\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{hastie2009elements}
\citation{hoerl1970ridge}
\citation{tibshirani1996lasso}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sec:introduction}{{1}{1}{Introduction}{section.1}{}}
\citation{hoerl1970ridge}
\@writefile{toc}{\contentsline {section}{\numberline {2}Theory and methods}{2}{section.2}}
\newlabel{sec:theory}{{2}{2}{Theory and methods}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Linear Regression}{2}{subsection.2.1}}
\newlabel{eq_model}{{1}{2}{Linear Regression}{equation.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Ordinary least squares}{2}{subsection.2.2}}
\newlabel{eq:ols}{{2}{2}{Ordinary least squares}{equation.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Ridge regression}{2}{subsection.2.3}}
\newlabel{eq:ridge_cost}{{3}{2}{Ridge regression}{equation.2.3}{}}
\citation{tibshirani1996lasso}
\citation{hastie2009elements}
\citation{james2013introduction}
\newlabel{eq:ridge}{{4}{3}{Ridge regression}{equation.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Lasso regression}{3}{subsection.2.4}}
\newlabel{eq:lasso}{{5}{3}{Lasso regression}{equation.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Confidence interval of model parameters}{3}{subsection.2.5}}
\newlabel{eq:confidence}{{6}{3}{Confidence interval of model parameters}{equation.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Evaluation scores}{3}{subsection.2.6}}
\newlabel{eq:mse}{{7}{3}{Evaluation scores}{equation.2.7}{}}
\newlabel{eq:r2}{{8}{3}{Evaluation scores}{equation.2.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}k-fold cross validation}{3}{subsection.2.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8}Bootstrap method for resampling}{3}{subsection.2.8}}
\citation{hastie2009elements}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces The k-fold cross-validation algorithm.\relax }}{4}{algocf.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:k-fold}{{1}{4}{k-fold cross validation}{algocf.1}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces The bootstrap algorithm.\relax }}{4}{algocf.2}}
\newlabel{alg:bootstrap}{{2}{4}{Bootstrap method for resampling}{algocf.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.9}The bias-variance trade-off}{4}{subsection.2.9}}
\newlabel{eq:bias-variance}{{9}{4}{The bias-variance trade-off}{equation.2.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.10}Franke's function and digital terrain data}{4}{subsection.2.10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.11}Implementation}{5}{subsection.2.11}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{5}{section.3}}
\newlabel{sec:results}{{3}{5}{Results}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Regression on Franke's function}{5}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}$\beta $-values confidence interval}{5}{subsubsection.3.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The $\beta $-values and their 95\% confidence interval with $m=5$ and $\sigma =0.1$ for OLS used on data generated with Franke's function.\relax }}{5}{figure.caption.3}}
\newlabel{fig:betaconfidence}{{1}{5}{The $\beta $-values and their 95\% confidence interval with $m=5$ and $\sigma =0.1$ for OLS used on data generated with Franke's function.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}MSE and $R^2$ of OLS, with and without resampling}{6}{subsubsection.3.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces MSE of the model as a function of degree $m$. The model is trained on data generated with Franke's function, with n=20, noise $\sigma $ = 0.1. In the case where no resampling is done (MSE-train), the MSE flattens out at a low value. When applying the bootstrap method (MSE-test) for resampling however, the MSE begins to rise after reaching a certain model complexity, creating a minimum point where the error is the lowest.\relax }}{6}{figure.caption.4}}
\newlabel{fig:mseVSdegreeOLS}{{2}{6}{MSE of the model as a function of degree $m$. The model is trained on data generated with Franke's function, with n=20, noise $\sigma $ = 0.1. In the case where no resampling is done (MSE-train), the MSE flattens out at a low value. When applying the bootstrap method (MSE-test) for resampling however, the MSE begins to rise after reaching a certain model complexity, creating a minimum point where the error is the lowest.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Bias-Variance-tradeoff}{6}{subsubsection.3.1.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces $R^2$-score of the model as a function of degree $m$. The model is trained on data generated with Franke's function, with n=20, noise $\sigma $ = 0.1. In the case where no resampling is done (R2-train), the $R^2$ increases and then flattens out. When applying the k-fold cross validation method (R2-test) for resampling however, the $R^2$ begins to decrease after reaching a certain model complexity, creating a maximum point where the $R^2$-score is closest to 1 (the optimal value).\relax }}{6}{figure.caption.5}}
\newlabel{fig:r2VSdegreeOLS}{{3}{6}{$R^2$-score of the model as a function of degree $m$. The model is trained on data generated with Franke's function, with n=20, noise $\sigma $ = 0.1. In the case where no resampling is done (R2-train), the $R^2$ increases and then flattens out. When applying the k-fold cross validation method (R2-test) for resampling however, the $R^2$ begins to decrease after reaching a certain model complexity, creating a maximum point where the $R^2$-score is closest to 1 (the optimal value).\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Bias, variance and MSE for the ordinary least square method on the Franke's function data set with $n=20$ and noise $\sigma =0.1$, resampled with the bootstrap method, as a function of model complexity/polynomial degree. The bias starts of high and decreases as the model complexity increases, whilst the variance grows with the model complexity.\relax }}{7}{figure.caption.6}}
\newlabel{fig:biasvarianceOLS}{{4}{7}{Bias, variance and MSE for the ordinary least square method on the Franke's function data set with $n=20$ and noise $\sigma =0.1$, resampled with the bootstrap method, as a function of model complexity/polynomial degree. The bias starts of high and decreases as the model complexity increases, whilst the variance grows with the model complexity.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}Resampling with Ridge-regression}{7}{subsubsection.3.1.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.5}Resampling with Lasso-regression}{7}{subsubsection.3.1.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces MSE for the Ridge regression method on the Franke's function data set with $n=20$ and noise $\sigma =0.1$, resampled with the bootstrap method, as a function of model complexity/polynomial degree. The different lines in the plot represent different values of the hyperparameter $\lambda $. When $\lambda =0$ it is equivalent to the OLS-method.\relax }}{7}{figure.caption.7}}
\newlabel{fig:lambdavsdegreesRIDGE}{{5}{7}{MSE for the Ridge regression method on the Franke's function data set with $n=20$ and noise $\sigma =0.1$, resampled with the bootstrap method, as a function of model complexity/polynomial degree. The different lines in the plot represent different values of the hyperparameter $\lambda $. When $\lambda =0$ it is equivalent to the OLS-method.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces MSE for the Lasso regression method on the Franke's function data set with $n=20$ and noise $\sigma =0.1$, resampled with the bootstrap method, as a function of model complexity/polynomial degree. The different lines in the plot represent different values of the hyperparameter $\lambda $\relax }}{7}{figure.caption.8}}
\newlabel{fig:lambdavsdegreesLASSO}{{6}{7}{MSE for the Lasso regression method on the Franke's function data set with $n=20$ and noise $\sigma =0.1$, resampled with the bootstrap method, as a function of model complexity/polynomial degree. The different lines in the plot represent different values of the hyperparameter $\lambda $\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.6}Comparing MSE of the different regression methods}{8}{subsubsection.3.1.6}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces The smalles MSE for the different regressions methods used on the Franke's function dataset, and the hyperparameter $\lambda $ and the degree $m$ that results the MSE in question. Produced using the bootstrap resampling method.\relax }}{8}{table.caption.9}}
\newlabel{tab:minerrorFRANKE}{{1}{8}{The smalles MSE for the different regressions methods used on the Franke's function dataset, and the hyperparameter $\lambda $ and the degree $m$ that results the MSE in question. Produced using the bootstrap resampling method.\relax }{table.caption.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.7}Terrain data}{8}{subsubsection.3.1.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces MSE for the OLS regression method on the reduced terrain data set. The MSE decreases as the degree increases, and flattens out for degrees higher than about 15\relax }}{8}{figure.caption.10}}
\newlabel{fig:mseVSdegreeOLSterrain}{{7}{8}{MSE for the OLS regression method on the reduced terrain data set. The MSE decreases as the degree increases, and flattens out for degrees higher than about 15\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces MSE for the Ridge regression method on the reduced terrain data set. Different colours represents different hyperparameters $\lambda $. The MSE decreases as the degree increases. The minimum MSE decreases as $\lambda $ decreases.\relax }}{8}{figure.caption.11}}
\newlabel{fig:mseVSdegreeLASSOterrain}{{8}{8}{MSE for the Ridge regression method on the reduced terrain data set. Different colours represents different hyperparameters $\lambda $. The MSE decreases as the degree increases. The minimum MSE decreases as $\lambda $ decreases.\relax }{figure.caption.11}{}}
\citation{hastie2009elements}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces MSE for the Lasso regression method on the reduced terrain data set. Different colours represents different hyperparameters $\lambda $. The MSE decreases as the degree increases. The minimum MSE decreases as $\lambda $ decreases.\relax }}{9}{figure.caption.12}}
\newlabel{fig:mseVSdegreeRIDGEterrain}{{9}{9}{MSE for the Lasso regression method on the reduced terrain data set. Different colours represents different hyperparameters $\lambda $. The MSE decreases as the degree increases. The minimum MSE decreases as $\lambda $ decreases.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{9}{section.4}}
\newlabel{sec:discussion}{{4}{9}{Discussion}{section.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{10}{section.5}}
\newlabel{sec:conclusion}{{5}{10}{Conclusion}{section.5}{}}
\bibstyle{plainnat}
\bibdata{ourbib}
\bibcite{hastie2009elements}{{1}{2009}{{Hastie et~al.}}{{Hastie, Tibshirani, and Friedman}}}
\bibcite{hoerl1970ridge}{{2}{1970}{{Hoerl and Kennard}}{{}}}
\bibcite{james2013introduction}{{3}{2013}{{James et~al.}}{{James, Witten, Hastie, and Tibshirani}}}
\bibcite{tibshirani1996lasso}{{4}{1996}{{Tibshirani}}{{}}}
\@input{appendix.aux}
