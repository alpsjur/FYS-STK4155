\section{Conclusion}
\label{sec:conclusion}

% Basically abstract in different wording

In summary, we looked at the OLS, Ridge, and Lasso methods of linear regression, and compared them with each other for two different sets of data: a analytical function, the Franke function, and real terrain data of the inner Oslo fjord. The Franke function looks like idealised terrain data, and so it was natural to compare the methods for each data set. For OLS, we could observe a bias-variance trade-off when resampling using the Bootstrap algorithm. For the Franke function, we found that ridge gave the smallest MSE of $0.014$, outperforming the other two methods, especially Lasso, which yielded a MSE of $0.043$. As a bias-variance trade-off was observed, it was not surprising that the ridge algorithm outperformed OLS, as Ridge reduces variance at the cost of a penalty to bias. The Lasso algorithm was outperformed possibly because of too high a tolerance (lack of computing power).

Performing the same analysis on the real terrain data, we observed no bias-variance trade-off, implying that the data set was even more complex than a 35 degree polynomial. As such it was never overfitted, and so the variance did not decrease. Comparing our methods on this data set, we found that OLS performed better than both Ridge and Lasso, with a MSE of around 2000. This was expected as we did not observe a bias-variance trade-off. The complexity of the data made ridge and lasso less feasible, as the variance never increased, i.e. the data did not become overfitted, even for OLS. The complexity of our terrain data may be due to a sharp trough at the beachline, and so more careful handling of the fjord topography could have resulted in a simpler data set.
