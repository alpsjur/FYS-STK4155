\section{Introduction}
\label{sec:introduction}

% utkast til innledning
% kilde: r√¶va mi (trenger kilder)
The use of machine learning for problem solving has risen in popularity as large data sets have become available for analysis. There now exists many different methods in varying complexity for both supervised and unsupervised learning. All of these methods have advantages and drawbacks, as well as many similarities. This means that we can get familiar with some of the central themes in machine learning by studying simple algorithms, such as different linear regression schemes. A notable example is the bias-variance trade-off, where it is observed that the total mean-square-error (MSE) of a model starts off high, decreases until a minimum, and increases again as discussed by \citet{hastie2009elements}. This effect is known to be rather general, but there are also quirks related to each regression algorithm.

While using the Ordinary Least Squares (OLS) is straightforward, the Ridge and Lasso algorithms must be tuned using a hyperparameter, see \citet{hoerl1970ridge}, and \citet{tibshirani1996lasso} respectively. In particular, the algorithms may perform differently depending on the data we analyse, which will be a central theme in this report.

Starting with the Theory and methods section, we will describe three different algorithms for linear regression, as well as our resampling and cross-validation techniques and the bias-variance trade-off. In the Results section we will show our selected figures and data, with a focus on comparisons between the different methods. Moving on to the Discussion section we will consider the compared values and try to conclude which method seems to be most fit for fitting terrain data. We will also argue why that is the case. Finally, concluding in the Conclusion section, we will summarise the most important results as well as our thoughts around them.