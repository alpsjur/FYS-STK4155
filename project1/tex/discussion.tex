\section{Discussion}
\label{sec:discussion}

% Discuss beta confidence intervals
\subsubsection*{Behaviour of the different regression methods applied to Franke's function}
Let's first look at the $\boldsymbol{\beta}$ confidence interval in Figure \ref{fig:biasvarianceOLS}. As we can see, the range of values that has a 95\% probability of containing $\beta_i$ is quite wide, for some in the same order of magnitude as the $\beta$-value itself. Considering the expression for the interval, stated in (\ref{eq:confidence}), we see that the width of the interval strongly depends on the spread of $\boldsymbol{\epsilon}$ in (\ref{eq_model}). As the noise term $\boldsymbol{\epsilon}$ goes to zero, so does the width of the confidence interval.

% Discuss R2 and MSE values for the different methods
Looking at the  the mean squared error and the $R^2$-value of the approximation to Franke's function using ordinary least squares (Figure \ref{fig:mseVSdegreeOLS} and \ref{fig:r2VSdegreeOLS}), we see the difference in the results when using the entire training set as test data, and when separating the data into test and training data using resampling. Comparing the two results, they both display the same trend: When train and test data is not separated, the approximation seems to unambiguously become better with increased model complexity, though, converging to what looks to be the minimum error. When using designated test data, however, it becomes apparent that using the training data for testing gives a false sense of security, as the mean squared error now starts increasing and the $R^2$-value decreasing when a certain model complexity is reached. It appears that a complexity given by a polynomial of degree $m \in [5,7]$ gives the best result for both the $R^2$-value and the MSE when using OLS in this case.

From equation \eqref{eq:bias-variance} we expect the total error to be the sum of the bias squared, the variance of model, plus some irreducible error. Looking at Figure \ref{fig:biasvarianceOLS}, showing the MSE, variance and bias for the ordinary least squared method on Franke's function, we see that the results displayed correspond nicely with these expectations. Thus we observe how bias and variance gives rise to the shape of the MSE curve, both in Figure \ref{fig:biasvarianceOLS} and \ref{fig:mseVSdegreeOLS}. As discussed in lectures, and explained in \cite{hastie2009elements}, a higher variance for a more complex polynomial is a consequence of over fitting, allowing the polynomials to fit to the noise in the data. When only using the training data as test data, this over fitting goes unnoticed, which is the reason for the behaviour of the training data curve in Figure \ref{fig:mseVSdegreeOLS}.

Understanding the importance of proper use of test data and resampling, we move on to compare the now discussed ordinary least squares method to our two other regression methods, Ridge regression and Lasso regression. We have here, as can be seen in Figure \ref{fig:lambdavsdegreesRIDGE} and \ref{fig:lambdavsdegreesLASSO}, chosen to focus on comparing the MSE of the methods, as opposed to the $R^2$-score simply because we found it more intuitive.

Moving into the territory of regression methods with hyperparameters, we study the behaviour of the mean squared error of the approximations for various values of $\lambda$ and varying complexity.
Common for both Lasso and Ridge is the almost constant behaviour of the MSE after reaching a certain model complexity, even when a designated test set is used. This stands in contrast to the convex shape of the OLS-MSE in Figure \ref{fig:mseVSdegreeOLS}. By adding the penalty term in (\ref{eq:ridge_cost}) and (\ref{eq:lasso}), we introduce a bias to the model. However, the penalty term also prevents over-fitting, and thus reduces the model variance.

Focusing on the behaviour of the MSE for different tuning parameters $\lambda$, it appears that the model preforms better for smaller $\lambda$.

Looking at the minimum MSE values of both Ridge and Lasso, the first preforms somewhat better than the latter on Franke's function. Both OLS and Ridge reaches a minimum MSE of about 0.015, with Ridge performing slightly better. Lasso, on the other hand, has a minimum MSE around 0.021. Why Lasso preforms noticeably worse is not obvious, but could be due to us not finding the right value of convergence tolerance or some other factor. The exact numbers of the MSE will also vary with the random noise added to Franke's function. We have chosen a specific seed to evaluate, but the trend was the same regarding what seed was used.
% Discuss hyperparameters lambda and alpha


% Discuss difference between fitting Franke function and real terrain data
\subsubsection*{Performance on real terrain data}
Looking at the performance of the different regression methods for the reduced terrain data, a challenge arises. For ridge and lasso regression, the computational power required for polynomials of degree $m > 20$ is so great that the time needed to compute them is unreasonably high for a regular laptop. A consequence of this is that we don't have the data that is necessary in order to directly compare OLS to ridge and lasso for degrees like $m$ around 30. From the behaviour of Ridge and Lasso on Franke's function, and our expectations from theoretical knowledge, we can however, predict that the behaviour of the MSE will continue the trend that we see for the degrees of which we have data.

For OLS, we don't get the same convex shape in the plot of the MSE as for Franke's function. As we have seen, the increase in MSE for Franke's function is due to an increasing model variance, i.e. overfitting the model to the training data. In the case of the real terrain data, we believe that the data is so complex, and demanding such a high order of polynomial to be represented, that the data does not become overfitted, even for polynomials up to 35th order. Taking a look at Ridge and Lasso, we see that the minimum MSE decreases as $\lambda$ decreases, as we also found for Franke's function. However, since we did not experience overfitting with OLS, introducing a penalty term does not decrease the variance, and we do not get a better result with the Ridge or Lasso regression. 
