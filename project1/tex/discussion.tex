\section{Discussion}
\label{sec:discussion}

% Discuss beta confidence intervals
\subsubsection*{Behaviour of the different regression methods applied to Franke's function}
Let's first look at the $\boldsymbol{\beta}$ confidence interval in Figure \ref{fig:biasvarianceOLS}. As we can see, the range of values that has a 95\% probability of containing $\beta_i$ is quite wide, for some in the same order of magnitude as the $\beta$-value itself. Considering the expression for the interval, stated in (\ref{eq:confidence}), we see that the width of the interval strongly depends on the spread of $\boldsymbol{\epsilon}$ in (\ref{eq_model}). As the noise term $\boldsymbol{\epsilon}$ goes to zero, so does the width of the confidence interval.  

% Discuss R2 and MSE values for the different methods
Looking at the  the mean squared error and the $R^2$-value of the approximation to Franke's function using ordinary least squares (Figure \ref{fig:mseVSdegreeOLS} and \ref{fig:r2VSdegreeOLS}), we see the difference in the results when using the entire training set as test data, and when separating the data into test and training data using resampling. Comparing the two results, they both display the same trend: When train and test data is not separated, the approximation seems to unambiguously become better with increased model complexity, though, converging to what looks to be the minimum error. When using designated test data, however, it becomes appearent that using the training data for testing gives a false sense of security, as the mean squared error now starts increasing and the $R^2$-value decreasing when a certain model complexity is reached. It appears that a complexity given by a polynomial of degree $m \in [5,7]$ gives the best result for both the $R^2$-value and the MSE when doing ordinary least squares regression in this case.

From equation \eqref{eq:bias-variance} we expect the total error to be the sum of the bias squared, the variance of model, pluss some irreducible error. Looking at figure \ref{fig:biasvarianceOLS}, showing the MSE, variance and bias for the ordinary least squared method on Franke's function, we see that the results displayed correspond nicely with these expectations. Thus we observe how bias and variance gives rise to the shape of the MSE curve, both in figure \ref{fig:biasvarianceOLS} and \ref{fig:mseVSdegreeOLS}. As discussed in lectures, and explained in \cite{hastie2009elements}, a higher variance for a more complex polynomial is a consequence of overfitting, allowing the polynomials to fit to the noice in the data. When only using the training data as test data, this overfitting goes unnoticed, which is the reason for the behaviour of the training data curve in Figure \ref{fig:mseVSdegreeOLS}.

Understanding the importance of proper use of test data and resampling, we move on to compare the now dicussed ordinary least squares method to our two other regression methods, ridge regression and lasso regression. We have here, as can be seen in figure \ref{fig:lambdavsdegreesRIDGE} and \ref{fig:lambdavsdegreesLASSO}, chosen to focus on comparing the MSE of the methods, as opposed to the $R^2$-score simply because we found it more intuitive.

Moving into the territory of regression methods with hyperparameters, we study the behaviour of the mean squared error of the approximations for various values of $\lambda$ and varying complexity. 
Common for both Lasso and Ridge is the almost constant behaviour of the MSE after reaching a certain model complexity, even when a designated test set is used. This stands in contrast to the convex shape of the OLS-MSE in Figure \ref{fig:mseVSdegreeOLS}. By adding the penalty term in (\ref{eq:ridge_cost}) and (\ref{eq:lasso}), we introduce a bias to the model. However, the penalty term also prevents over-fitting, and thus reduces the model variance. 

Focusing on the behaviour of the MSE for different tuning parameters $\lambda$, it appears that the model preforms better for smaller $\lambda$. 

Looking at the minimum MSE values of both Ridge and Lasso, the first preforms considerable better than the latter on Franke's function. Both OLS and Ridge reaches a minimum MSE of about 0.0015, with Ridge preforming slightly better. Lasso, on the other hand, has a minimum MSE almost three times as large. Why Lasso preforms so much worse is not obvious, but could be due to ... noe med learning rate, at metoden ikke konvergerer osv. 
% Discuss hyperparameters lambda and alpha


% Discuss difference between fitting Franke function and real terrain data
\subsubsection*{Performance on real terrain data}