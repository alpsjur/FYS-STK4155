\section{Discussion}
\label{sec:discussion}

% Discuss beta confidence intervals
\subsubsection*{Behaviour of the different regression methods applied to Franke's function}

Noe om $\beta$ konfidensintervall omtrent her....\\

% Discuss R2 and MSE values for the different methods
Looking at the  the mean squared error and the $R^2$-value of the approximation to Franke's function using ordinary least squares (figure \ref{fig:mseVSdegreeOLS} and \ref{fig:r2VSdegreeOLS}), we see the difference in the results when using the entire training set as test data, and when separating the data into test and training data using resampling. Comparing the two results, they both display the same trend: When train and test data is not separated the approximation seems to unambiguously become better with increased model complexity, though, converging to what looks to be the minimum error. When using designated test data, however, it becomes appearent that using the training data for testing gives a false sense of security, as the mean squared error now starts increasing and the $R^2$-value decreasing when a certain model complexity is reached. It appears that a complexity given by a polynomial of degree $m \in [5,7]$ gives the best result for both the $R^2$-value and the MSE when doing ordinary least squares regression in this case.

From equation \eqref{eq:bias-variance} we expect the total error to be the sum of the bias squared, the variance of model, and the variance of the noice. Looking at figure \ref{fig:biasvarianceOLS}, showing the MSE, variance and bias for the ordinary least squared method on Franke's function, we see that the results displayed correspond nicely with these expectations. Thus we observe how bias and variance gives rise to the shape of the MSE curve, both in figure \ref{fig:biasvarianceOLS} and \ref{fig:mseVSdegreeOLS}. As discussed in lectures, and explained in \cite{hastie2009elements}, a higher variance for a more complex polynomial is a consequence of overfitting, allowing the polynomials to fit to the noice in the data. When only using the training data as test data, this overfitting goes unnoticed, which is the reason for the behaviour of the training data curve in figure \ref{fig:mseVSdegreeOLS}.

Understanding the importance of proper use of test data and resampling, we move on to compare the now dicussed ordinary least squares method to our two other regression methods, ridge regression and lasso regression. We have here, as can be seen in figure \ref{fig:lambdavsdegreesRIDGE} and \ref{fig:lambdavsdegreesLASSO}, chosen to focus on comparing the MSE of the methods, as opposed to the $R^2$-score simply because we found it more intuitive.

Moving into the territory of regression methods with hyperparameters, we study the behaviour of the mean squared error of the approximations for various values of $\lambda$ and varying complexity.
% Discuss hyperparameters lambda and alpha

% Discuss difference between fitting Franke function and real terrain data
