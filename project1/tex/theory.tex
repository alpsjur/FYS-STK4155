\section{Theory}
\label{sec:theory}

\subsection{Linear Regression}
A linear Regression model makes a prediction of the response variable $y_i$ by computing a weighted sum of the explanatory variables:

\begin{equation*}
	\tilde{y}_i = \beta_0x_{i0} + \beta_1x_{i1} + \ldots + \beta_{p-1}x_{ip-1} 
\end{equation*}
Here, $\tilde{y}_i$ is the prediction, $\{\beta_j\}_{j=0}^{p-1}$ are the regression parameters, while $p$ are the number of explanatory variables. 

In vectorized form, this can be written as

\begin{equation*}
	\boldsymbol{\tilde{y}} = \boldsymbol{X}\boldsymbol{\beta}
\end{equation*}
where $\boldsymbol{\tilde{y}}=[\tilde{y}_0,\tilde{y}_1,\ldots, \tilde{y}_{n-1}]^T$ are the predicted values, $\boldsymbol{\beta}=[\beta_0, \beta_1, \ldots . \beta_{n-1}]^T$ are the regression parameters,  and $\boldsymbol{X}$ is the so called design matrix given by  

\[\boldsymbol{X} =
\mqty[x_{00} & x_{01} & \hdots & x_{0p-1} \\
x_{10} & x_{11} & \hdots & x_{1p-1} \\
\vdots & \ddots & \ddots &  \vdots \\
x_{n0} & \hdots & \hdots & x_{np-1}],
\]
where $n$ is the number of cases. 

In this project, we are dealing with a two dimensional problem, where each row of the design matrix represents the variables of a $m$th order polynomial, i.e. is on the form $[\{x^iy^j : i+y\leq m\}]$

In order to compute the regression parameters, a cost function $C(\boldsymbol{\beta})$ is introduced. The $\boldsymbol{\beta}$ that are used will then be the ones that minimize the cost function. Different cost functions gives rise to different regression methods. Here we will look at Ordinary least squares, Ridge and Lasso regression. 

\subsection{Ordinary least squares}
One form of the cost function is given as
\begin{align*}
	C(\boldsymbol{\beta}) 
	&= \frac{1}{n}\sum_{n=0}^{n-1} \left(y_i-\tilde{y}_i\right)^2 \\
	&= \frac{1}{n}\sum_{n=0}^{n-1} \left(y_i-\boldsymbol{x}_{i*}\boldsymbol{\beta}\right)^2\\
	&=\frac{1}{n}\left(\left(\boldsymbol{y}-\boldsymbol{X\beta}\right)^T\left(\boldsymbol{y}-\boldsymbol{X\beta}\right)\right)
\end{align*}
where $\boldsymbol{x}_{i}$ is the $i$th row of the design matrix. By setting $\frac{\partial C(\boldsymbol{\beta})}{\partial\boldsymbol{\beta}}=0$, one can show that the model parameters that minimizes the cost function are given by
\begin{equation*}
	\boldsymbol{\beta}=\left(\boldsymbol{X}^T\boldsymbol{X}\right)^{-1}\boldsymbol{X}^T\boldsymbol{y}
\end{equation*}
These are the model parameters used in the Ordinary Least Squares (OLS) model. 

\subsection{Ridge regression}
Something something restrain

In Ridge regression, the cost function takes the form 
\begin{equation*}
	C(\boldsymbol{\beta}) 
	= \frac{1}{n}\sum_{n=0}^{n-1} \left(y_i-\boldsymbol{x}_{i*}\boldsymbol{\beta}\right)^2 + \lambda \sum_{i=0}^{p-1}\beta_i^2
\end{equation*}

Minimizing this results in model parameters on the form
\begin{equation*}
\boldsymbol{\beta}=\left(\boldsymbol{X}^T\boldsymbol{X}+\boldsymbol{\lambda}\boldsymbol{I}\right)^{-1}\boldsymbol{X}^T\boldsymbol{y}
\end{equation*}
	
\subsection{Lasso regression}

\begin{equation*}
C(\boldsymbol{\beta}) 
= \frac{1}{n}\sum_{n=0}^{n-1} \left(y_i-\boldsymbol{x}_{i*}\boldsymbol{\beta}\right)^2 + \lambda \sum_{i=0}^{p-1}\left|\beta\right|
\end{equation*}

\subsection{Score function}

\subsection{k-fold cross validation}
There are several methods for estimating the skill of a machine learning model. One such method is the $k$-fold cross-validation procedure, which can be used when working with a limited data sample. The idea is to divide the data sample into $k$ groups or folds, and then retain one of the folds to use as a test set after fitting a model to the remaining data. This is done for all folds. Algorithm \ref{alg:k-fold} outlines the different steps in the procedure. 

\begin{algorithm}[htbp]\caption{The $k$-fold cross-validation algorithm.}\label{alg:k-fold}
	\SetAlgoLined
	\BlankLine
	\BlankLine
	Shuffle the dataset randomly\;
	Divide the dataset into $k$ folds\;
	\ForEach{$k$}{
		Take the $k$th fold out to use as test data set\;
		Set the remaining folds as training data set\;
		Fit a model to the training set\;
		Evaluate the model on the test set\;
		Retain the evaluation score and discard the model\;
	}
	Calculate the mean of the evaluation scores\;	
	\BlankLine
	\BlankLine
\end{algorithm}

This yields a statistical estimate for how well the model will preform on new data. The choice of $k$ will however effect the bias and variance in the estimation of the evaluation scores. It has been shown empirically that $k=5$ or $k=10$ gives neither a high bias nor variance \citep{james2013introduction}. In this project, a value of 5 was chosen for $k$. 

\subsection{The bias-variance trade-off}

\subsection{Franke's function and digital terrain data}
In this project, the different regression methods were applied on both constructed and real data. The first was in the form of a sum of weighted exponentials, known as Franke's function:

\begin{align*}
	f\left(x,y\right) &=\frac{3}{4}\exp\left(-\frac{\left(9x-2\right)^2}{4}-\frac{\left(9y-2\right)^2}{4} \right) \\
	&+\frac{3}{4}\exp\left(-\frac{\left(9x+1\right)^2}{49}-\frac{\left(9y+1\right)^2}{10} \right) \\
	&+\frac{1}{2}\exp\left(-\frac{\left(9x-7\right)^2}{4}-\frac{\left(9y-3\right)^2}{4} \right) \\
	&-\frac{1}{5}\exp\left(-\left(9x-4\right)^2-\left(9y-7\right)^2 \right)	
\end{align*}

In addition to the above terms, a normal distributed noise term with $\mu = 0$ and $\sigma^2 = 1$ was added. 

After testing the code on the simpler Franke's function, the same regression methods were used and evaluated on real digital terrain data downloaded from \url{https://earthexplorer.usgs.gov/}. The data used in this project is of the Oslo fjord region. 


