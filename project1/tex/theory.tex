\section{Theory and methods}
\label{sec:theory}

\subsection{Linear Regression}
We consider a dataset with $n$ cases consisting of the response variable $\boldsymbol{y}=[y_0,y_1,\ldots,y_{n-1}]^T$ and some explanatory variables. The assumption is then made that $\boldsymbol{y}$ can be explained as a functional relationship on the form
\begin{equation}\label{eq_model}
	\boldsymbol{y} = {f}(\cdot) + \boldsymbol{\epsilon}
\end{equation}
Here, $\boldsymbol{\epsilon}$ is assumed to be normally distributed with mean 0 and variance $\sigma^2$.

A linear Regression model is built on the assumption that ${f}(\cdot)$ is a linear mapping from the explanatory variables $x_{ij}$ to the response variable $y_i$, given by computing a weighted sum of the explanatory variables:

\begin{equation*}
	\tilde{y}_i = \beta_0x_{i0} + \beta_1x_{i1} + \ldots + \beta_{p-1}x_{ip-1}
\end{equation*}
Here, $i=0,1,\ldots ,n-1$, $\tilde{y}_i$ is the prediction, $\{\beta_j\}_{j=0}^{p-1}$ are the regression parameters, while $p$ is the number of explanatory variables.

In vectorized form, this can be written as

\begin{equation*}
	\boldsymbol{\tilde{y}} = \boldsymbol{X}\boldsymbol{\beta}
\end{equation*}
where $\boldsymbol{\tilde{y}}=[\tilde{y}_0,\tilde{y}_1,\ldots, \tilde{y}_{n-1}]^T$ are the predicted values, $\boldsymbol{\beta}=[\beta_0, \beta_1, \ldots . \beta_{n-1}]^T$ are the regression parameters,  and $\boldsymbol{X}$ is the so called design matrix given by

\[\boldsymbol{X} =
\mqty[x_{00} & x_{01} & \hdots & x_{0p-1} \\
x_{10} & x_{11} & \hdots & x_{1p-1} \\
\vdots & \ddots & \ddots &  \vdots \\
x_{n0} & \hdots & \hdots & x_{np-1}],
\]

In this project, we are dealing with a two dimensional problem, where each row of the design matrix represents the variables of a $m$-th order polynomial, i.e. is on the form $[\{x^iy^j : i+j\leq m\}]$

In order to compute the regression parameters, a cost function $C(\boldsymbol{\beta})$ is introduced. The $\boldsymbol{\beta}$ is then defined as the minimization of the cost function. Different cost functions gives rise to different regression methods. Here we will look at Ordinary Least Squares, Ridge and Lasso regression.

\subsection{Ordinary least squares}
One form of the cost function is given as
\begin{align*}
	C(\boldsymbol{\beta})
	&= \frac{1}{n}\sum_{n=0}^{n-1} \left(y_i-\tilde{y}_i\right)^2 \\
	&= \frac{1}{n}\sum_{n=0}^{n-1} \left(y_i-\boldsymbol{x}_{i*}\boldsymbol{\beta}\right)^2\\
	&=\frac{1}{n}\left(\left(\boldsymbol{y}-\boldsymbol{X\beta}\right)^T\left(\boldsymbol{y}-\boldsymbol{X\beta}\right)\right)
\end{align*}
where $\boldsymbol{x}_{i}$ is the $i$th row of the design matrix. By setting $\frac{\partial C(\boldsymbol{\beta})}{\partial\boldsymbol{\beta}}=0$, one can show that the model parameters that minimizes the cost function are given by
\begin{equation}\label{eq:ols}
	\boldsymbol{\beta}=\left(\boldsymbol{X}^T\boldsymbol{X}\right)^{-1}\boldsymbol{X}^T\boldsymbol{y}
\end{equation}
These are the model parameters used in the Ordinary Least Squares (OLS) model.

\subsection{Ridge regression}
A problem that can occur, especially when dealing with a large design matrix $\boldsymbol{X}$, is that the columns of $\boldsymbol{X}$ are linearly dependent, causing $\boldsymbol{X}^T\boldsymbol{X}$ to be singular. \cite{hoerl1970ridge} proposed a solution to the singularity problem by introducing a tuning parameter $\lambda$. This tuning parameter is added to the diagonal elements of  $\boldsymbol{X}^T\boldsymbol{X}$, and thus causing the matrix to be non-singular.
In Ridge regression, the cost function then takes the form
\begin{equation}\label{eq:ridge_cost}
	C(\boldsymbol{\beta})
	= \frac{1}{n}\sum_{n=0}^{n-1} \left(y_i-\boldsymbol{x}_{i*}\boldsymbol{\beta}\right)^2 + \lambda \sum_{i=0}^{p-1}\beta_i^2
\end{equation}
where $\lambda \in \left[0,\infty\right)$. The last term of (\ref{eq:ridge_cost}) is often called the penalty term.
Minimizing this results in model parameters on the form
\begin{equation}\label{eq:ridge}
\boldsymbol{\beta}=\left(\boldsymbol{X}^T\boldsymbol{X}+\boldsymbol{\lambda}\boldsymbol{I}\right)^{-1}\boldsymbol{X}^T\boldsymbol{y}
\end{equation}
Here, $\boldsymbol{I}$ is the identity matrix.. The parameter $\lambda$ then has to be tuned, for example by the use of cross-validation.
One can see that by setting $\lambda = 0$, (\ref{eq:ridge}) simplifies to the OLS solution in (\ref{eq:ols}).

\subsection{Lasso regression}
The choice of the penalty term in (\ref{eq:ridge_cost}) is somewhat arbitrary, and other penalty functions could be considered. \cite{tibshirani1996lasso} introduced a penalty function such that the cost function takes the form

\begin{equation*}
C(\boldsymbol{\beta})
= \frac{1}{n}\sum_{n=0}^{n-1} \left(y_i-\boldsymbol{x}_{i*}\boldsymbol{\beta}\right)^2 + \lambda \sum_{i=0}^{p-1}\left|\beta\right|
\end{equation*}

\subsection{Variance of model parameters}
Dette skj√∏nner jeg ikke.
\begin{equation*}
	Var\left(\boldsymbol{\beta}\right) = \sigma^2\left(\boldsymbol{X}^T\boldsymbol{X}\right)^{-1}
\end{equation*}

\subsection{Evaluation scores}
In this project, we will use two well known expressions to calculate the error in the predicted values. That is the Mean Square Error (MSE)
\begin{equation}\label{eq:mse}
	MSE(\boldsymbol{y},\boldsymbol{\tilde{y}}) = \frac{1}{n} \sum_{i=0}^{n-1}\left(y_i-\tilde{y}_i\right)^2
\end{equation}
and the $R^2$ score function
\begin{equation}\label{eq:r2}
	R^2(\boldsymbol{y},\boldsymbol{\tilde{y}}) = 1- \frac{\sum_{i=0}^{n-1}\left(y_i-\tilde{y}_i\right)^2}{\sum_{i=0}^{n-1}\left(y_i-\overline{y}\right)^2}
\end{equation}
Here, the mean value $\overline{y}$ is given as
\begin{equation*}
	\overline{y}=\frac{1}{n}\sum_{i=0}^{n-1}y_i
\end{equation*}

\subsection{k-fold cross validation}
There are several methods for estimating the skill of a machine learning model. One such method is the $k$-fold cross-validation procedure, which can be used when working with a limited data sample. The idea is to divide the data sample into $k$ groups or folds, and then retain one of the folds to use as a test set after fitting a model to the remaining data. This is done for all folds. Algorithm \ref{alg:k-fold} outlines the different steps in the procedure.

\begin{algorithm}[htbp]\caption{The $k$-fold cross-validation algorithm.}\label{alg:k-fold}
	\SetAlgoLined
	\BlankLine
	\BlankLine
	Shuffle the dataset randomly\;
	Divide the dataset into $k$ folds\;
	\ForEach{$k$}{
		Take the $k$th fold out to use as test data set\;
		Set the remaining folds as training data set\;
		Fit a model to the training set\;
		Evaluate the model on the test set\;
		Retain the evaluation score and discard the model\;
	}
	Calculate the mean of the evaluation scores\;
	\BlankLine
	\BlankLine
\end{algorithm}

This yields a statistical estimate for how well the model will preform on new data. The choice of $k$ will however affect the bias and variance in the estimation of the evaluation scores. It has been shown empirically that $k=5$ or $k=10$ gives neither a high bias nor variance \citep{james2013introduction}. In this project, a value of 5 was chosen for $k$.

\subsection{Bootstrap method for resampling}
Another procedure for estimating the skill of a model is the Bootstrap method for resampling. In Bootstrap, a sample is drawn, with replacement, from the data, and the model is fitted to the sample. This is done multiple times, and for each cycle, the fitted model makes a prediction on a test set. Finally, all the predictions are evaluated. Algorithm \ref{alg:bootstrap} gives an overview of the bootstrap method.
\begin{algorithm}[htbp]\caption{The bootstrap algorithm.}\label{alg:bootstrap}
	\SetAlgoLined
	\BlankLine
	\BlankLine
	Split the data into training and test sets\;
	Set a number of bootstrap samples $n$\;
	Set a sample size $N$\;
	\ForEach{$n$}{
		Draw a $N$-sized sample with replacement from the training set\;
		Fit a model to the data sample\;
		Apply the model on the test set\;
	}
	Evaluate all the model predictions\;
	Calculate the mean of the evaluation scores\;
	\BlankLine
	\BlankLine
\end{algorithm}

The sample size $N$ was set to the same size as the train set in this project. 

\subsection{The bias-variance trade-off}
Taking a look at the MSE again, (\ref{eq:mse}) can be expressed as the expectation value of $\left(\boldsymbol{y}-\boldsymbol{\tilde{y}}\right)^2$, which can be decomposed as follows. For the full derivation, see \eqref{eq:derivation_bias_variance}.
\begin{align}\label{eq:bias-variance}
\begin{split}
	\mathds{E}\left[\left(\boldsymbol{y}-\boldsymbol{\tilde{y}}\right)^2\right]
	=&\frac{1}{n}\sum_{i=0}^{n-1}\left(f_i-\mathds{E}\left[{\tilde{y}_i}\right]\right)^2 \\
	+&\frac{1}{n}\sum_{i=0}^{n-1}\left(\tilde{y}_i-\mathds{E}\left[{\tilde{y}_i}\right]\right)^2 + \sigma^2 \\
	=& Bias^2 + Variance + \sigma^2
\end{split}
\end{align}
Here, $f_i$ comes from (\ref{eq_model}) written out element wise as $y_i = f_i + \epsilon_i$. The first term in (\ref{eq:bias-variance}) is the squared bias, while the second term is the variance of the model. The last term, $\sigma^2$, comes from the assumption of a normal distributed noise in (\ref{eq_model}), and is the so called irreducible error. This error is beyond our control, even if the true value of $f_i$ is known.

When the complexity of the model, i. e. the order of the polynomial, increases, the squared bias tends to decrease. For the variance, the opposite tends to happen \citep{hastie2009elements}.

\subsection{Franke's function and digital terrain data}
In this project, the different regression methods were applied on both constructed and real data. The first was in the form of a sum of weighted exponentials, known as Franke's function:

\begin{align*}
	f\left(x,y\right) &=\frac{3}{4}\exp\left(-\frac{\left(9x-2\right)^2}{4}-\frac{\left(9y-2\right)^2}{4} \right) \\
	&+\frac{3}{4}\exp\left(-\frac{\left(9x+1\right)^2}{49}-\frac{\left(9y+1\right)^2}{10} \right) \\
	&+\frac{1}{2}\exp\left(-\frac{\left(9x-7\right)^2}{4}-\frac{\left(9y-3\right)^2}{4} \right) \\
	&-\frac{1}{5}\exp\left(-\left(9x-4\right)^2-\left(9y-7\right)^2 \right)
\end{align*}

In addition to the above terms, a normal distributed noise term with $\mu = 0$ and $\sigma^2 = 1$ was added.

After testing the code on the simpler Franke's function, the same regression methods were used and evaluated on real digital terrain data downloaded from \url{https://earthexplorer.usgs.gov/}. The data used in this project is of the Oslo fjord region.

\subsection{Implementation}
Since both OLS and Ridge gives an explicit expression for $\boldsymbol{\beta}$, the model parameters can be calculated directly. In this project, expression (\ref{eq:ols}) and (\ref{eq:ridge}) was calculated using the linear algebra functionality in the Python package \texttt{numpy}.

Unlike OLS and Ridge regression, there is no general, explicit expression for the model parameters in Lasso regression. Therefore, functionalities from the \texttt{scikit-learn} package was used to calculate $\boldsymbol{\beta}$  in the Lasso regression case.



All plots were made using the Python package \texttt{matplotlib}.
