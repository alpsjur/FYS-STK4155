\section{Theory}
\label{sec:theory}

\subsection{Ordinary least squares}

\subsection{Ridge regression}
	
\subsection{Lasso regression}

\subsection{Mean squared error}

\subsection{Score function}

\subsection{k-fold cross validation}
There are several methods for estimating the skill of a machine learning model. One such method is the $k$-fold cross-validation procedure, which can be used when working with a limited data sample. The idea is to divide the data sample into $k$ groups or folds, and then retain one of the folds to use as a test set after fitting a model to the remaining data. This is done for all folds. Algorithm \ref{alg:k-fold} outlines the different steps in the procedure. 

\begin{algorithm}[htbp]\caption{The $k$-fold cross-validation algorithm.}\label{alg:k-fold}
	\SetAlgoLined
	\BlankLine
	\BlankLine
	Shuffle the dataset randomly\;
	Divide the dataset into $k$ folds\;
	\ForEach{$k$}{
		Take the $k$th fold out to use as test data set\;
		Set the remaining folds as training data set\;
		Fit a model to the training set\;
		Evaluate the model on the test set\;
		Retain the evaluation score and discard the model\;
	}
	Calculate the mean of the evaluation scores\;	
	\BlankLine
	\BlankLine
\end{algorithm}

This yields a statistical estimate for how well the model will preform on new data. The choice of $k$ will however effect the bias and variance in the estimation of the evaluation scores. It has been shown empirically that $k=5$ or $k=10$ gives neither a high bias nor variance \citep{james2013introduction}. In this project, a value of 5 was chosen for $k$. 


% Eksempel for store matriser
\[A =
\mqty[b_1 & c_1 & 0 & \hdots & \hdots & 0 \\
a_1 & b_2 & c_2 & 0 & \hdots & 0 \\
0 & a_2 & b_3 & c_3 & \hdots & 0 \\
\vdots & \ddots & \ddots & \ddots & \ddots & \vdots \\
0 & \hdots & \ddots & a_{n-2} & b_{n-1} & c_{n-1} \\
0 & \hdots & \hdots & 0 & a_{n-1} & b_n],
\]