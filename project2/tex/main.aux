\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{yeh2009UCI}
\citation{prosjekt1}
\citation{prosjekt1}
\citation{yeh2009UCI}
\citation{yeh2009UCI}
\citation{yeh2009UCI}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sec:introduction}{{1}{1}{Introduction}{section.1}{}}
\citation{prosjekt1}
\citation{LogRegLectures}
\@writefile{toc}{\contentsline {section}{\numberline {2}Theory and methods}{2}{section.2}}
\newlabel{sec:theory}{{2}{2}{Theory and methods}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Data sets}{2}{subsection.2.1}}
\newlabel{sec:datasets}{{2.1}{2}{Data sets}{subsection.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Logistic Regression}{2}{subsection.2.2}}
\newlabel{eq:sigmoid}{{1}{2}{Logistic Regression}{equation.2.1}{}}
\citation{GDLectures}
\newlabel{eq:prob_relation}{{2.2}{3}{Logistic Regression}{equation.2.1}{}}
\newlabel{eq:log_p}{{2}{3}{Logistic Regression}{equation.2.2}{}}
\newlabel{eq:cost_function}{{3}{3}{Logistic Regression}{equation.2.3}{}}
\newlabel{eq:cost_d}{{4}{3}{Logistic Regression}{equation.2.4}{}}
\newlabel{eq:prob_ratio}{{5}{3}{Logistic Regression}{equation.2.5}{}}
\newlabel{eq:pBx}{{6}{3}{Logistic Regression}{equation.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Gradient Descent Methods}{3}{subsection.2.3}}
\newlabel{eq:gradient_decent}{{7}{3}{The General Idea}{equation.2.7}{}}
\newlabel{eq:gradient_sum}{{8}{3}{Stochastic Gradient Descent}{equation.2.8}{}}
\citation{Nielsen}
\newlabel{eq:stochstic_gradient_descent}{{9}{4}{Stochastic Gradient Descent}{equation.2.9}{}}
\newlabel{eq:learning_schedule}{{10}{4}{Dynamic learning rate}{equation.2.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Neural Networks}{4}{subsection.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Schematic diagram of a neural network with five input neurons in the input layer, one hidden layer with tree neurons and a single output neuron in the output layer.\relax }}{4}{figure.caption.5}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:NNstructure}{{1}{4}{Schematic diagram of a neural network with five input neurons in the input layer, one hidden layer with tree neurons and a single output neuron in the output layer.\relax }{figure.caption.5}{}}
\citation{wang2018classification}
\newlabel{eq:forward}{{11}{5}{Forward feeding}{equation.2.11}{}}
\newlabel{leakyReLU}{{12}{5}{Forward feeding}{equation.2.12}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces The forward feeding algorithm.\relax }}{5}{algocf.1}}
\newlabel{alg:forward}{{1}{5}{Forward feeding}{algocf.1}{}}
\citation{prosjekt1}
\citation{Nielsen}
\newlabel{eq:cross-entropy}{{13}{6}{Backpropagation}{equation.2.13}{}}
\newlabel{eq:quadratic_cost}{{14}{6}{Backpropagation}{equation.2.14}{}}
\newlabel{eq:backprop}{{15}{6}{Backpropagation}{equation.2.15}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces The backpropagation algorithm.\relax }}{6}{algocf.2}}
\newlabel{alg:backprop}{{2}{6}{Backpropagation}{algocf.2}{}}
\citation{prosjekt1}
\newlabel{eq:w_update}{{16}{7}{Overfitting and Regularization}{equation.2.16}{}}
\newlabel{eq:b_update}{{17}{7}{Overfitting and Regularization}{equation.2.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Quality of Measurements}{7}{subsection.2.5}}
\newlabel{eq:acc}{{18}{7}{Quality of Measurements}{equation.2.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Implementation}{7}{subsection.2.6}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{7}{section.3}}
\newlabel{sec:results}{{3}{7}{Results}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Tuning Learning Rate and Minibatch Size}{7}{subsection.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Heatmap showing the AUC of the Logistic Regression applied on the classification problem for minibatch sizes $m\in [5, 5000]$ and learning rates $\gamma \in [0.5\cdot 10^{-3}, 0.5]$ in logarithmic scale. AUC increases from blue to yellow.\relax }}{8}{figure.caption.12}}
\newlabel{fig:TuneLogReg_auc}{{2}{8}{Heatmap showing the AUC of the Logistic Regression applied on the classification problem for minibatch sizes $m\in [5, 5000]$ and learning rates $\gamma \in [0.5\cdot 10^{-3}, 0.5]$ in logarithmic scale. AUC increases from blue to yellow.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Heatmap showing the accuracy of the Logistic Regression applied on the classification problem for minibatch sizes $m\in [5, 5000]$ and learning rates $\gamma \in [0.5\cdot 10^{-3}, 0.5]$ in logarithmic scale. Accuracy increases from blue to yellow.\relax }}{8}{figure.caption.13}}
\newlabel{fig:TuneLogReg_accuracy}{{3}{8}{Heatmap showing the accuracy of the Logistic Regression applied on the classification problem for minibatch sizes $m\in [5, 5000]$ and learning rates $\gamma \in [0.5\cdot 10^{-3}, 0.5]$ in logarithmic scale. Accuracy increases from blue to yellow.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Heatmap showing the AUC of the Neural Network applied on the classification problem for minibatch sizes $m\in [1, 1000]$ and learning rates $\gamma \in [10^{-2.5}, 10^{0.3}]$ in logarithmic scale. The hidden layers were configured as $[20, 20]$. AUC increases from blue to yellow.\relax }}{8}{figure.caption.15}}
\newlabel{fig:TuneNN_auc}{{4}{8}{Heatmap showing the AUC of the Neural Network applied on the classification problem for minibatch sizes $m\in [1, 1000]$ and learning rates $\gamma \in [10^{-2.5}, 10^{0.3}]$ in logarithmic scale. The hidden layers were configured as $[20, 20]$. AUC increases from blue to yellow.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Heatmap showing the accuracy of the Neural Network applied on the classification problem for minibatch sizes $m\in [1, 1000]$ and learning rates $\gamma \in [10^{-2.5}, 10^{0.3}]$ in logarithmic scale. The hidden layers were configured as $[20, 20]$. Accuracy increases from blue to yellow.\relax }}{9}{figure.caption.16}}
\newlabel{fig:TuneNN_accuracy}{{5}{9}{Heatmap showing the accuracy of the Neural Network applied on the classification problem for minibatch sizes $m\in [1, 1000]$ and learning rates $\gamma \in [10^{-2.5}, 10^{0.3}]$ in logarithmic scale. The hidden layers were configured as $[20, 20]$. Accuracy increases from blue to yellow.\relax }{figure.caption.16}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Table of parameters that gave the highest \textbf  {AUC} in the classification problem. Learning rate $\gamma $, minibatch size m, accuracy acc and AUC is shown for Logistic Regression and Neural Network.\relax }}{9}{table.caption.18}}
\newlabel{tab:AUC}{{1}{9}{Table of parameters that gave the highest \textbf {AUC} in the classification problem. Learning rate $\gamma $, minibatch size m, accuracy acc and AUC is shown for Logistic Regression and Neural Network.\relax }{table.caption.18}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Table of parameters that gave the highest \textbf  {accuracy} in the classification problem. Learning rate $\gamma $, minibatch size m, accuracy acc and AUC is shown for Logistic Regression and Neural Network.\relax }}{9}{table.caption.19}}
\newlabel{tab:acc}{{2}{9}{Table of parameters that gave the highest \textbf {accuracy} in the classification problem. Learning rate $\gamma $, minibatch size m, accuracy acc and AUC is shown for Logistic Regression and Neural Network.\relax }{table.caption.19}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Fraction of true and false negatives and positives for Logistic regression and Neural Network applied on the classification problem, when optimal parameters are used.\relax }}{9}{table.caption.20}}
\newlabel{tab:confusion}{{3}{9}{Fraction of true and false negatives and positives for Logistic regression and Neural Network applied on the classification problem, when optimal parameters are used.\relax }{table.caption.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Neural Networks for Regression on Franke's Function}{9}{subsection.3.2}}
\citation{yeh2009UCI}
\citation{yeh2009UCI}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Heatmap showing the MSE of the Neural Network applied on Franke's function for minibatch sizes $m\in [1, 1000]$ and learning rates $\gamma \in [10^{-4}, 1]$ in logarithmic scale. The hidden layers were configured as $[100, 20]$. MSE decreases from blue to yellow. Minimum MSE and maximum R2-score was achieved with $m=1$ and $\gamma =0.146780$.\relax }}{10}{figure.caption.21}}
\newlabel{fig:TuneNN_mse}{{6}{10}{Heatmap showing the MSE of the Neural Network applied on Franke's function for minibatch sizes $m\in [1, 1000]$ and learning rates $\gamma \in [10^{-4}, 1]$ in logarithmic scale. The hidden layers were configured as $[100, 20]$. MSE decreases from blue to yellow. Minimum MSE and maximum R2-score was achieved with $m=1$ and $\gamma =0.146780$.\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{10}{section.4}}
\newlabel{sec:discussion}{{4}{10}{Discussion}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Tuning to optimal parameters}{10}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Comparing the classifiers}{10}{subsubsection.4.1.1}}
\citation{prosjekt1}
\bibstyle{plainnat}
\bibdata{ourbib}
\bibcite{GDLectures}{{1}{2019{a}}{{Hjort-Jensen}}{{}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Franke?}{11}{subsection.4.2}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces The minimum MSE achieved when applying different regression models and Neural Network on Franke's function.\relax }}{11}{table.caption.22}}
\newlabel{tab:mse}{{4}{11}{The minimum MSE achieved when applying different regression models and Neural Network on Franke's function.\relax }{table.caption.22}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{11}{section.5}}
\newlabel{sec:conclusion}{{5}{11}{Conclusion}{section.5}{}}
\bibcite{LogRegLectures}{{2}{2019{b}}{{Hjort-Jensen}}{{}}}
\bibcite{prosjekt1}{{3}{2019}{{J\IeC {\o }rgensen et~al.}}{{J\IeC {\o }rgensen, Sjur, and Kallmyr}}}
\bibcite{Nielsen}{{4}{2015}{{Nielsen}}{{}}}
\bibcite{wang2018classification}{{5}{2018}{{Wang et~al.}}{{Wang, Phillips, Sui, Liu, Yang, and Cheng}}}
\bibcite{yeh2009UCI}{{6}{2009}{{Yeh and Lien}}{{}}}
