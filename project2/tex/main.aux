\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{LogRegLectures}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sec:introduction}{{1}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Theory and methods}{1}{section.2}}
\newlabel{sec:theory}{{2}{1}{Theory and methods}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Logistic Regression}{1}{subsection.2.1}}
\newlabel{eq:sigmoid}{{1}{1}{Logistic Regression}{equation.2.1}{}}
\newlabel{eq:prob_relation}{{2.1}{1}{Logistic Regression}{equation.2.1}{}}
\citation{GDLectures}
\newlabel{eq:log_p}{{2}{2}{Logistic Regression}{equation.2.2}{}}
\newlabel{eq:cost_function}{{3}{2}{Logistic Regression}{equation.2.3}{}}
\newlabel{eq:cost_d}{{4}{2}{Logistic Regression}{equation.2.4}{}}
\newlabel{eq:prob_ratio}{{5}{2}{Logistic Regression}{equation.2.5}{}}
\newlabel{eq:pBx}{{6}{2}{Logistic Regression}{equation.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Gradient Descent Methods}{2}{subsection.2.2}}
\newlabel{eq:gradient_decent}{{7}{2}{The General Idea}{equation.2.7}{}}
\newlabel{eq:gradient_sum}{{8}{2}{Stochastic Gradient Descent}{equation.2.8}{}}
\citation{Nielsen}
\newlabel{eq:stochstic_gradient_descent}{{9}{3}{Stochastic Gradient Descent}{equation.2.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Neural networks}{3}{subsection.2.3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:NNstructure}{{\caption@xref {fig:NNstructure}{ on input line 154}}{3}{The structure of a network}{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Schematic diagram of a neural network with five input neurons in the input layer, one hidden layer with tree neurons and a single output neuron in the output layer.\relax }}{3}{figure.caption.4}}
\newlabel{fig:NNstructure}{{1}{3}{Schematic diagram of a neural network with five input neurons in the input layer, one hidden layer with tree neurons and a single output neuron in the output layer.\relax }{figure.caption.4}{}}
\citation{wang2018classification}
\newlabel{eq:forward}{{10}{4}{Forward feeding}{equation.2.10}{}}
\newlabel{leakyReLU}{{11}{4}{Forward feeding}{equation.2.11}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces The forward feeding algorithm.\relax }}{4}{algocf.1}}
\newlabel{alg:forward}{{1}{4}{Forward feeding}{algocf.1}{}}
\newlabel{eq:cross-entropy}{{12}{4}{Backpropagation}{equation.2.12}{}}
\newlabel{quadratic cost}{{13}{4}{Backpropagation}{equation.2.13}{}}
\citation{prosjekt1}
\citation{yeh2009UCI}
\citation{yeh2009UCI}
\citation{yeh2009UCI}
\newlabel{eq:backprop}{{14}{5}{Backpropagation}{equation.2.14}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces The backpropagation algorithm.\relax }}{5}{algocf.2}}
\newlabel{alg:backprop}{{2}{5}{Backpropagation}{algocf.2}{}}
\newlabel{eq:w_update}{{15}{5}{Overfitting and Regularization}{equation.2.15}{}}
\newlabel{eq:b_update}{{16}{5}{Overfitting and Regularization}{equation.2.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Data sets}{5}{subsection.2.4}}
\citation{prosjekt1}
\citation{yeh2009UCI}
\citation{prosjekt1}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Quality of Measurements}{6}{subsection.2.5}}
\newlabel{eq:acc}{{17}{6}{Quality of Measurements}{equation.2.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Heatmap showing the AUC of the Logistic Regression for different values of the minibatch sizes and initial learning rates.\relax }}{6}{figure.caption.12}}
\newlabel{fig:TuneLogReg_auc}{{2}{6}{Heatmap showing the AUC of the Logistic Regression for different values of the minibatch sizes and initial learning rates.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Heatmap showing the accuracy of the Logistic Regression for different values of the minibatch sizes and initial learning rates.\relax }}{6}{figure.caption.13}}
\newlabel{fig:TuneLogReg_accuracy}{{3}{6}{Heatmap showing the accuracy of the Logistic Regression for different values of the minibatch sizes and initial learning rates.\relax }{figure.caption.13}{}}
\bibstyle{plainnat}
\bibdata{ourbib}
\bibcite{GDLectures}{{1}{2019{a}}{{Hjort-Jensen}}{{}}}
\bibcite{LogRegLectures}{{2}{2019{b}}{{Hjort-Jensen}}{{}}}
\bibcite{prosjekt1}{{3}{2019}{{J\IeC {\o }rgensen et~al.}}{{J\IeC {\o }rgensen, Sjur, and Kallmyr}}}
\bibcite{Nielsen}{{4}{2015}{{Nielsen}}{{}}}
\bibcite{wang2018classification}{{5}{2018}{{Wang et~al.}}{{Wang, Phillips, Sui, Liu, Yang, and Cheng}}}
\bibcite{yeh2009UCI}{{6}{2009}{{Yeh and Lien}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{7}{section.3}}
\newlabel{sec:results}{{3}{7}{Results}{section.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.0.1}Tuning Learning Rate and Minibatch Size}{7}{subsubsection.3.0.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Verification by Comparison to Scikit-Learn}{7}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Neural Networks for Regression on Franke's Function}{7}{subsection.3.2}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Fraction of true and false negatives and positives for Neural network (NN), Logistic regression (LR) and SciKitLearn's neural network (SKL)\relax }}{7}{table.caption.15}}
\newlabel{tab:confusion}{{1}{7}{Fraction of true and false negatives and positives for Neural network (NN), Logistic regression (LR) and SciKitLearn's neural network (SKL)\relax }{table.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \relax }}{7}{figure.caption.16}}
\newlabel{fig:}{{4}{7}{\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{7}{section.4}}
\newlabel{sec:discussion}{{4}{7}{Discussion}{section.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{7}{section.5}}
\newlabel{sec:conclusion}{{5}{7}{Conclusion}{section.5}{}}
