\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{prosjekt1}
\citation{prosjekt1}
\citation{yeh2009UCI}
\citation{yeh2009UCI}
\citation{yeh2009UCI}
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:introduction}{{1}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Theory and methods}{1}{section.2}\protected@file@percent }
\newlabel{sec:theory}{{2}{1}{Theory and methods}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Data sets}{1}{subsection.2.1}\protected@file@percent }
\newlabel{sec:datasets}{{2.1}{1}{Data sets}{subsection.2.1}{}}
\citation{prosjekt1}
\citation{LogRegLectures}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Logistic Regression}{2}{subsection.2.2}\protected@file@percent }
\newlabel{eq:sigmoid}{{1}{2}{Logistic Regression}{equation.2.1}{}}
\newlabel{eq:prob_relation}{{2.2}{2}{Logistic Regression}{equation.2.1}{}}
\citation{GDLectures}
\newlabel{eq:log_p}{{2}{3}{Logistic Regression}{equation.2.2}{}}
\newlabel{eq:cost_function}{{3}{3}{Logistic Regression}{equation.2.3}{}}
\newlabel{eq:cost_d}{{4}{3}{Logistic Regression}{equation.2.4}{}}
\newlabel{eq:prob_ratio}{{5}{3}{Logistic Regression}{equation.2.5}{}}
\newlabel{eq:pBx}{{6}{3}{Logistic Regression}{equation.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Gradient Descent Methods}{3}{subsection.2.3}\protected@file@percent }
\newlabel{eq:gradient_decent}{{7}{3}{The General Idea}{equation.2.7}{}}
\newlabel{eq:gradient_sum}{{8}{3}{Stochastic Gradient Descent}{equation.2.8}{}}
\citation{Nielsen}
\newlabel{eq:stochstic_gradient_descent}{{9}{4}{Stochastic Gradient Descent}{equation.2.9}{}}
\newlabel{eq:learning_schedule}{{10}{4}{Dynamic learning rate}{equation.2.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Neural Networks}{4}{subsection.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Schematic diagram of a neural network with five input neurons in the input layer, one hidden layer with tree neurons and a single output neuron in the output layer.\relax }}{4}{figure.caption.5}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:NNstructure}{{1}{4}{Schematic diagram of a neural network with five input neurons in the input layer, one hidden layer with tree neurons and a single output neuron in the output layer.\relax }{figure.caption.5}{}}
\citation{wang2018classification}
\newlabel{eq:forward}{{11}{5}{Forward feeding}{equation.2.11}{}}
\newlabel{leakyReLU}{{12}{5}{Forward feeding}{equation.2.12}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces The forward feeding algorithm.\relax }}{5}{algocf.1}\protected@file@percent }
\newlabel{alg:forward}{{1}{5}{Forward feeding}{algocf.1}{}}
\newlabel{eq:cross-entropy}{{13}{5}{Backpropagation}{equation.2.13}{}}
\citation{prosjekt1}
\newlabel{quadratic cost}{{14}{6}{Backpropagation}{equation.2.14}{}}
\newlabel{eq:backprop}{{15}{6}{Backpropagation}{equation.2.15}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces The backpropagation algorithm.\relax }}{6}{algocf.2}\protected@file@percent }
\newlabel{alg:backprop}{{2}{6}{Backpropagation}{algocf.2}{}}
\newlabel{eq:w_update}{{16}{6}{Overfitting and Regularization}{equation.2.16}{}}
\newlabel{eq:b_update}{{17}{6}{Overfitting and Regularization}{equation.2.17}{}}
\citation{prosjekt1}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Quality of Measurements}{7}{subsection.2.5}\protected@file@percent }
\newlabel{eq:acc}{{18}{7}{Quality of Measurements}{equation.2.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Implementation}{7}{subsection.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{7}{section.3}\protected@file@percent }
\newlabel{sec:results}{{3}{7}{Results}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Tuning Learning Rate and Minibatch Size}{7}{subsection.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Heatmap showing the AUC of the Logistic Regression for different values of the minibatch sizes and initial learning rates.\relax }}{7}{figure.caption.12}\protected@file@percent }
\newlabel{fig:TuneLogReg_auc}{{2}{7}{Heatmap showing the AUC of the Logistic Regression for different values of the minibatch sizes and initial learning rates.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Heatmap showing the accuracy of the Logistic Regression for different values of the minibatch sizes and initial learning rates.\relax }}{8}{figure.caption.13}\protected@file@percent }
\newlabel{fig:TuneLogReg_accuracy}{{3}{8}{Heatmap showing the accuracy of the Logistic Regression for different values of the minibatch sizes and initial learning rates.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Heatmap showing the AUC of the Neural Network for different values of the minibatch sizes and learning rates.\relax }}{8}{figure.caption.15}\protected@file@percent }
\newlabel{fig:TuneNN_auc}{{4}{8}{Heatmap showing the AUC of the Neural Network for different values of the minibatch sizes and learning rates.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Heatmap showing the accuracy of the Neural Network for different values of the minibatch sizes and learning rates.\relax }}{8}{figure.caption.16}\protected@file@percent }
\newlabel{fig:TuneNN_accuracy}{{5}{8}{Heatmap showing the accuracy of the Neural Network for different values of the minibatch sizes and learning rates.\relax }{figure.caption.16}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Table of parameters that gave the highest \textbf  {AUC} in the classification problem. Learning rate $\gamma $, minibatch size m, accuracy acc and AUC is shown for Logistic Regression and Neural Network.\relax }}{9}{table.caption.18}\protected@file@percent }
\newlabel{tab:AUC}{{1}{9}{Table of parameters that gave the highest \textbf {AUC} in the classification problem. Learning rate $\gamma $, minibatch size m, accuracy acc and AUC is shown for Logistic Regression and Neural Network.\relax }{table.caption.18}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Table of parameters that gave the highest \textbf  {accuracy} in the classification problem. Learning rate $\gamma $, minibatch size m, accuracy acc and AUC is shown for Logistic Regression and Neural Network.\relax }}{9}{table.caption.19}\protected@file@percent }
\newlabel{tab:acc}{{2}{9}{Table of parameters that gave the highest \textbf {accuracy} in the classification problem. Learning rate $\gamma $, minibatch size m, accuracy acc and AUC is shown for Logistic Regression and Neural Network.\relax }{table.caption.19}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Fraction of true and false negatives and positives for Logistic regression, Neural network, and SciKit Learn applied on the classification problem.\relax }}{9}{table.caption.20}\protected@file@percent }
\newlabel{tab:confusion}{{3}{9}{Fraction of true and false negatives and positives for Logistic regression, Neural network, and SciKit Learn applied on the classification problem.\relax }{table.caption.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Verification by Comparison to Scikit-Learn}{9}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Neural Networks for Regression on Franke's Function}{9}{subsection.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Heatmap showing the MSE of the Neural Network applied on Franke's function for minibatch sizes $m\in [1, 1000]$ and learning rates $\gamma \in [10^{-4}, 1]$. MSE decreases from blue to yellow.\relax }}{9}{figure.caption.21}\protected@file@percent }
\newlabel{fig:TuneNN_mse}{{6}{9}{Heatmap showing the MSE of the Neural Network applied on Franke's function for minibatch sizes $m\in [1, 1000]$ and learning rates $\gamma \in [10^{-4}, 1]$. MSE decreases from blue to yellow.\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{9}{section.4}\protected@file@percent }
\newlabel{sec:discussion}{{4}{9}{Discussion}{section.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{9}{section.5}\protected@file@percent }
\newlabel{sec:conclusion}{{5}{9}{Conclusion}{section.5}{}}
\bibstyle{plainnat}
\bibdata{ourbib}
\bibcite{GDLectures}{{1}{2019{a}}{{Hjort-Jensen}}{{}}}
\bibcite{LogRegLectures}{{2}{2019{b}}{{Hjort-Jensen}}{{}}}
\bibcite{prosjekt1}{{3}{2019}{{J\IeC {\o }rgensen et~al.}}{{J\IeC {\o }rgensen, Sjur, and Kallmyr}}}
\bibcite{Nielsen}{{4}{2015}{{Nielsen}}{{}}}
\bibcite{wang2018classification}{{5}{2018}{{Wang et~al.}}{{Wang, Phillips, Sui, Liu, Yang, and Cheng}}}
\bibcite{yeh2009UCI}{{6}{2009}{{Yeh and Lien}}{{}}}
