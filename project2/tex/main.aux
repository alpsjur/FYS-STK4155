\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{LogRegLectures}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sec:introduction}{{1}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Theory and methods}{1}{section.2}}
\newlabel{sec:theory}{{2}{1}{Theory and methods}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Logistic Regression}{1}{subsection.2.1}}
\newlabel{eq:sigmoid}{{1}{1}{Logistic Regression}{equation.2.1}{}}
\newlabel{eq:prob_relation}{{2.1}{1}{Logistic Regression}{equation.2.1}{}}
\citation{GDLectures}
\newlabel{eq:log_p}{{2}{2}{Logistic Regression}{equation.2.2}{}}
\newlabel{eq:cost_function}{{3}{2}{Logistic Regression}{equation.2.3}{}}
\newlabel{eq:cost_d}{{4}{2}{Logistic Regression}{equation.2.4}{}}
\newlabel{eq:prob_ratio}{{5}{2}{Logistic Regression}{equation.2.5}{}}
\newlabel{eq:pBx}{{6}{2}{Logistic Regression}{equation.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Gradient Descent Methods}{2}{subsection.2.2}}
\newlabel{eq:gradient_decent}{{7}{2}{The General Idea}{equation.2.7}{}}
\newlabel{eq:gradient_sum}{{8}{2}{Stochastic Gradient Descent}{equation.2.8}{}}
\citation{Nielsen}
\newlabel{eq:stochstic_gradient_descent}{{9}{3}{Stochastic Gradient Descent}{equation.2.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Neural networks}{3}{subsection.2.3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:NNstructure}{{\caption@xref {fig:NNstructure}{ on input line 154}}{3}{The structure of a network}{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Schematic diagram of a neural network with five input neurons in the input layer, one hidden layer with tree neurons and a single output neuron in the output layer.\relax }}{3}{figure.caption.4}}
\newlabel{fig:NNstructure}{{1}{3}{Schematic diagram of a neural network with five input neurons in the input layer, one hidden layer with tree neurons and a single output neuron in the output layer.\relax }{figure.caption.4}{}}
\citation{wang2018classification}
\newlabel{eq:forward}{{10}{4}{Forward feeding}{equation.2.10}{}}
\newlabel{leakyReLU}{{11}{4}{Forward feeding}{equation.2.11}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces The forward feeding algorithm.\relax }}{4}{algocf.1}}
\newlabel{alg:forward}{{1}{4}{Forward feeding}{algocf.1}{}}
\newlabel{eq:cross-entropy}{{12}{4}{Backpropagation}{equation.2.12}{}}
\citation{yeh2009UCI}
\citation{yeh2009UCI}
\citation{yeh2009UCI}
\citation{prosjekt1}
\bibstyle{plainnat}
\bibdata{ourbib}
\newlabel{eq:backprop}{{14}{5}{Backpropagation}{equation.2.14}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces The backpropagation algorithm.\relax }}{5}{algocf.2}}
\newlabel{alg:backprop}{{2}{5}{Backpropagation}{algocf.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Data sets}{5}{subsection.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Quality of Measurements}{5}{subsection.2.5}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{5}{section.3}}
\newlabel{sec:results}{{3}{5}{Results}{section.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.0.1}Tuning Learning Rate and Minibatch Size}{5}{subsubsection.3.0.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Verification by Comparison to Scikit-Learn}{5}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Neural Networks for Regression on Franke's Function}{5}{subsection.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{5}{section.4}}
\newlabel{sec:discussion}{{4}{5}{Discussion}{section.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{5}{section.5}}
\newlabel{sec:conclusion}{{5}{5}{Conclusion}{section.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Heatmap showing the accuracy of the Logistic Regression for different values of the minibatch sizes and initial learning rates.\relax }}{6}{figure.caption.11}}
\newlabel{fig:TuneLogReg}{{2}{6}{Heatmap showing the accuracy of the Logistic Regression for different values of the minibatch sizes and initial learning rates.\relax }{figure.caption.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Fraction of true and false negatives and positives for Neural network (NN), Logistic regression (LR) and SciKitLearn's neural network (SKL)\relax }}{6}{table.caption.13}}
\newlabel{tab:confusion}{{1}{6}{Fraction of true and false negatives and positives for Neural network (NN), Logistic regression (LR) and SciKitLearn's neural network (SKL)\relax }{table.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \relax }}{6}{figure.caption.14}}
\newlabel{fig:}{{3}{6}{\relax }{figure.caption.14}{}}
