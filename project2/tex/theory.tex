\section{Theory and methods}
\label{sec:theory}

\subsection{Logistic Regression}
Classification problems aim to predict the behaviour of a given object, and look for patterns based on discrete variables (i.e categories). Logistic regression can be used to solve such problems, commonly by the use of variables with binary outcomes such as true/false, positive/negative, success/failiure etc., or in the specific credit card case: \textit{risky/non-risky} \cite{LogRegLectures}

As opposed to linear regression, the equation one gets as a result of minimization of of the cost function by $\hat{\beta}$ using logistic regression, is non-linear, and is solved using minimization algorithms called \emph{gradient descent methods}. \\

When predicting the the output classes in which an object belongs, the prediction is based on the design matrix $\mathbf{\hat{X}} \in \mathbb{R}^{n\cross p}$ that contain $n$ samples that each carry $p$ features.

A distinction is made between \textit{hard classification} - deterministically determine the variable to a cathegory, and \textit{soft classification} - determines the probability that a given variable belongs in a certain cathegory. The latter is favorable in many cases, and logistic regression is the most used example og this type of classifier.

When using logistic regression, the probability that a given data point $x_i$ belongs in a cathegory $y_i$ is given by the Sigmoid-function (or logistic function):
\begin{equation}
\begin{split}
    & p(t) = \frac{1}{1 + e^{-t}} = \frac{e^t}{1+e^t} \\
    & 1-p(t) = p(-t)
\end{split}
  \label{eq:sigmoid}
\end{equation}

Assuming a binary classification problem, i.e. $y_i$ can be either 0 or 1, and a set of predictors $\hat{\beta}$ the Sigmoid function \eqref{eq:sigmoid} gives the probabilities with relation:
\begin{equation*}
  p(y_i = 0|x_i,\hat{\beta})  = 1 - p(y_i = 1|x_i,\hat{\beta})
  \label{eq:prob_relation}
\end{equation*}

The total likelihood for all possible outcomes $\mathcal{D}=\{(y_i,x_i)\}$ is used in the Maximum Likelihood Estimation (MLE), aiming at maximizing the log/likelihood funciton \eqref{eq:log_p}. The likelihood function can be expressed with $\mathcal{D}$:
\begin{equation*}
\begin{split}
    &\qquad P(\mathcal{D}|\hat{\beta}) = \\
    &\prod_{i=1}^n \qty[p(y_i = 1|x_i,\hat{\beta}) ]^{y_i}  \qty[1 - p(y_i = 0|x_i,\hat{\beta})]^{1-y_i}
\end{split}
\end{equation*}

And the log/likelihood function is then:
\begin{equation}
\begin{split}
    &P_{\log}(\hat{\beta}) = \\
    &\sum_{i=1}^n \Big(y_i \log \qty[p(y_i = 1|x_i,\hat{\beta})] \\
    &\quad+ (1-y_i) \log [1 - p(y_i = 0|x_i,\hat{\beta})]\Big)
\end{split}
\label{eq:log_p}
\end{equation}

The cost/error-function $\mathcal{C}$ (also called cross-entropy in statistics) is the negative of the log/likelihood. Maximizing $P_{\log}$ is thus the same as minimizing the cost function. The cost funciton is:
\begin{equation}
  \begin{split}
    &\mathcal{C}(\hat{\beta}) = - P_{\log}(\hat{\beta}) =  \\
    -&\sum_{i=1}^n \Big(y_i \log \qty[p(y_i = 1|x_i,\hat{\beta})] \\
    &\quad+ (1-y_i) \log [1 - p(y_i = 0|x_i,\hat{\beta})]\Big)
  \end{split}
  \label{eq:cost_function}
\end{equation}

Finding the parameters $\hat{\beta}$ that minimize the cost function is then done through derivation.
Defining the vector $\hat{y}$ containing $n$ elements $y_i$, the $n \cross p$ matrix $\hat{X}$ containing the $x_i$ elements, and the vector $\hat{p}$ that is the fittet probabilities $p(y_i|x_i,\hat{\beta})$, the first derivative of $\mathcal{C}$ is
\begin{equation}
  \nabla_\beta \mathcal{C} = \pdv{\mathcal{C}(\hat{\beta})}{\hat{\beta}} = - \hat{X}^T (\hat{y}-\hat{p})
  \label{eq:cost_d}
\end{equation}
This gives rise to set of linear equations, where the aim is to solve the system for $\hat{\beta}$.

With $\hat{x} = [1, x_1,x_2,...,x_p]$ and $p$ predictors $\hat{\beta} = [\beta_0,\beta_1,\beta_2,...,\beta_p]$ the ration between likelihoods of outcome is:
\begin{equation}
  \log \frac{p(\hat{\beta}\hat{x})}{1-p(\hat{\beta}\hat{x})} = \beta_0 + \beta_1x_1 + ... + \beta_px_p
  \label{eq:prob_ratio}
\end{equation}

\noindent and $p(\hat{\beta}\hat{x})$ defined by:
\begin{equation}
  p(\hat{\beta}\hat{x}) = \frac{e^{\beta_0 + \beta_1x_1 + ... + \beta_px_p}}{1+e^{\beta_0 + \beta_1x_1 + ... + \beta_px_p}}
  \label{eq:pBx}
\end{equation}

\subsection{Gradient Descent Methods}
\subsubsection*{The General Idea}
With the gradient of $\mathcal{C}$ defined as in \eqref{eq:cost_d}, we use this to find the minimum of the cost function. The basic idea is that by moving in the direction of the negative gradient of a function, we can move towards the value (in this case the $\beta$) that minimizes the function (in this case $\mathcal{C}(\beta)$). \cite{GDLectures}

This is done by repeating the algorithm
\begin{equation}
    \beta_{j+1} = \beta_j - \gamma \nabla_\beta \mathcal{C}(\beta) \quad j = 0,1,2,...
    \label{eq:gradient_decent}
\end{equation}
When a minimum is approached, $\nabla_\beta \mathcal{C}(\beta) \rightarrow$ 0, and thus we can set a limit when $\beta_{k+1} \approx \beta_k$ given a certain tolerance, and the $\beta$ which minimizes the cost funciton is found. $\gamma$ is in this case called the \textit{learning rate}, and is a parameter that must be tuned to each specific case in order to optimize the regression.

\subsubsection*{Stochastic Gradient Descent}
In this project we use a stochastic version of gradient descent, which is an improvement upon the regular gradient descent. This is done by expression the cost function (and thus also its gradient) as a sum
\begin{equation}
    \nabla_\beta \mathcal{C}(\beta) = \sum_i^n     \nabla_\beta c_i(\boldsymbol{x_i},\beta) ,
    \label{eq:gradient_sum}
\end{equation}
and by only taking calculating the gradient of a subset of the data at the time. These subsets, called \textit{minibatches} are of size $\boldsymbol{M}$ , and the total amount is $\frac{n}{\boldsymbol{M}}$ where $n$ is the amount of data points. The minibatches are denoted $\boldsymbol{B}_k$, with k = 1,2,...,$\frac{n}{\boldsymbol{M}}$.

Instead of a sum over all the the data points $i \in [1,n]$ we now in each step, sum over all the data points in the given minibatch $i \in \boldsymbol{B}_k$ where $k$ is picked randomly with uniform proabaility from $[1, \frac{n}{\boldsymbol{M}}]$.

The stochastic and final version of \eqref{eq:gradient_decent} is therefore given by the algorithm

\begin{equation}
    \beta_{j+1} = \beta_j - \gamma_j \sum_{i \in \boldsymbol{B}_k}\nabla_\beta c_i(\boldsymbol{x_i},\beta)
    \label{eq:stochstic_gradient_descent}
\end{equation}
An iteration over the total number of minibatches is commonly refered to as an \textit{epoch.}\\

By using the stochastic gradient descent method \eqref{eq:stochstic_gradient_descent} to minimize the cost function \eqref{eq:cost_function} we can this find the $\beta$ values that give the most accurate classification, by doing \textit{logistic regression}.

\subsection{Neural networks}
In this section, the equations used are based off the book by \cite{Nielsen}, unless otherwise specified.
\subsubsection*{The structure of a network}
Neural networks, as the name suggests, are inspired by our understanding of how networks of neurons function in the brain. As can be seen in the example network in Figure \ref{fig:NNstructure}, neurons are structured in layers. We always have a input and an output layer, in addition to a varying number of hidden layers. The input layer has as many neurons as there are input variables, while the output layer has one neuron for each output. How many neurons you have in the output layer depends on the specific problem. The number of neurons in each hidden layer, on the other hand, is not directly related to inputs or outputs, and must be decided in some other way.

As the diagram in Figure \ref{fig:NNstructure} suggests, the neurons in each layer are not connected with each other, but takes in inputs from the previous layer and passes on an output to the neurons in the next layer, as illustrated with arrows. This way, the inputs are fed through the network and processed, resulting in an output.
\begin{figure}[htbp]
	\centering
	\begin{tikzpicture}[
	plain/.style={
		draw=none,
		fill=none,
	},
	net/.style={
		matrix of nodes,
		nodes={
			draw,
			circle,
			inner sep=8pt
		},
		nodes in empty cells,
		column sep=0cm,
		row sep=-9pt
	},
	>=latex
	]
	\matrix[net] (mat)
	{
		|[plain]| \parbox{0.8cm}{\centering Input\\layer} & |[plain]| \parbox{0.8cm}{\centering Hidden\\layer} & |[plain]| \parbox{0.8cm}{\centering Output\\layer} \\
		& |[plain]| \\
		|[plain]| & \\
		& |[plain]| \\
		|[plain]| & |[plain]| \\
		& & \\
		|[plain]| & |[plain]| \\
		& |[plain]| \\
		|[plain]| & \\
		& |[plain]| \\    };
	\foreach \ai [count=\mi ]in {2,4,...,10}
	\draw[<-] (mat-\ai-1) -- node[above] {Input \mi} +(-1.9cm,0);
	\foreach \ai in {2,4,...,10}
	{\foreach \aii in {3,6,9}
		\draw[->] (mat-\ai-1) -- (mat-\aii-2);
	}
	\foreach \ai in {3,6,9}
	\draw[->] (mat-\ai-2) -- (mat-6-3);
	\draw[->] (mat-6-3) -- node[above] {Output} +(1.9cm,0);
	\label{fig:NNstructure}
	\end{tikzpicture}
	\caption{Schematic diagram of a neural network with five input neurons in the input layer, one hidden layer with tree neurons and a single output neuron in the output layer.}
	\label{fig:NNstructure}
\end{figure}

\subsubsection*{Forward feeding}
Each neuron has one or multiple inputs, as illustrated with arrows in Figure \ref{fig:NNstructure}. Each of these inputs has a weight associated with it. To clarify the notation used, let's take a look at the $j$th neuron in the $l$th layer. The weight associated with the input coming from the $k$th neuron in the previous layer is denoted as $w^l_{jk}$. In addition, each neuron has a bias associated with it, for the neuron in question denoted as $b^l_j$. Summing the weighted inputs and the bias, and feeding this to a function $f$, gives the activation $a^l_j$:
\begin{equation*}
	a^l_j = f\left(\left(\sum_{k}w^l_{jk}a^{l-1}_k\right) + b^l_j\right)
\end{equation*}
This activation is then fed forward as input to all the neuron in the next layer.

In matrix notation, the activation for the whole layer $l$ can be written as
\begin{equation}\label{eq:forward}
	\boldsymbol{a}^l = f\left(\boldsymbol{w}^l\boldsymbol{a}^{l-1}+\boldsymbol{b}^l\right)
\end{equation}
Here, $\boldsymbol{a}^l$ and $\boldsymbol{b}^l$ are vertical vectors containing the activations and biases of the $l$th layer, while $\boldsymbol{w}^l$ is a matrix with elements $w^l_{jk}$, i.e. the $j$th column contains the weights of the inputs reaching the $j$th neuron.

Let's look at the activation function in Eq. (\ref{eq:forward}) denoted with a $f$. In the case of classification, the sigmoid function stated in Eq. (\ref{eq:sigmoid}) is often used in introductory texts. As we will see in the backpropagation algorithm, the sigmoid is a good choice for activation function, since a small change in the output can be propagated backwards, resulting in small changes in the weights and biases through the network.

Another activation function is the so-called rectified linear unit (ReLU) function, given as $f(z)=\max{\left(0,z\right)}$ This gives an activation function which only fires when $z$ is positive. In the case of regression, a variant of ReLU, called leaky ReLU, was used in this project. This function is given as
\begin{equation}\label{leakyReLU}
  f(z) = \begin{cases}
            z &z\geq 0 \\
            0.01z &z<0 \\
         \end{cases}
\end{equation}
This form of ReLU has a small positive gradient for negative values, which allows for gradient based learning even when the $z$-value is negative \citep{wang2018classification}.

With a basis in Eq. (\ref{eq:forward}), the algorithm for forward feeding is given in Algorithm \ref{alg:forward}. Here $L$ is the total number of layers.
\begin{algorithm}[htbp]\caption{The forward feeding algorithm.}\label{alg:forward}
	\SetAlgoLined
	\BlankLine
	\BlankLine
	Set $\boldsymbol{a}^1$ = input\;
	\ForEach{l=2:L}{
    Compute $\boldsymbol{z}=\boldsymbol{w}^l\boldsymbol{a}^{l-1}+\boldsymbol{b}^l$\;
		Compute  $\boldsymbol{a}^l=f(\boldsymbol{z})$\;}
	Set output to $\boldsymbol{a}^L$\;
	\BlankLine
	\BlankLine
\end{algorithm}

Note that the output will have values between 0 and 1, when the sigmoid function is used to compute the activations of all the layers. In a classification problem, this corresponds to the likelihood of an outcome. For example, in a classification problem with five classes, the network would have five output neurons, each representing a class. The final classification of an input would then be the class with the highest probability.
In the regression case, we see that when using the leaky ReLU activation function from Eq. (\ref{leakyReLU}), the output can be any positive value (strictly also negative values, due to the leakage, but this would call for large negative $z$-values).

\subsubsection*{Backpropagation}
When training the network, the goal is to find the weights and biases that minimize the cost function $C$. In this project, the cross-entropy cost function was used for classification. For a single neuron, this is given as
\begin{equation}\label{eq:cross-entropy}
	C = -\sum_{i=1}^n\left[y_i\ln a^L_i + (1-y_i)\ln(1-a^L_i)\right]
\end{equation}
Here, we are summing over $n$ points of training data, where $a_i^L$ is the output with the corresponding correct value $y_i$.

In the regression case, the quadratic cost function, given as
\begin{equation}
  C = \frac{1}{2}\sum_{i=1}^n||y_i-a_i^L||^2
\end{equation}
is used.

To find the weights and biases that minimize Eq.(\ref{eq:cross-entropy}) and (\ref{quadratic cost}), one can use Stochastic Gradient Decent, as described previously. But in order to use SGD, the derivatives of $C$ with respect to all the weights and biases must be computed, and it is here that backpropagation comes in. It can be shown that the derivatives are given as in Eq. (\ref{eq:backprop}).
\begin{equation}\label{eq:backprop}
\begin{aligned}
	\delta^L &= \nabla_aC\odot f'(z^L)\\
	\delta^l &= ((\boldsymbol{w}^{l+1})^T\delta^{l+1})\odot f'(\boldsymbol{z}^l) \\
	\frac{\partial C}{\partial b^l_j} &= \delta_j^l \\
	\frac{\partial C}{\partial w_{jk}^l} &= a_k^{l-1}\delta^l_j
\end{aligned}
\end{equation}

Taking a look at the expression for $\delta^L$, one can show that this reduces to $\delta^L = \boldsymbol{a}-\boldsymbol{y}$, when the cross-entropy cost function is combined with the sigmoid activation function, and similarly for the quadratic cost and ReLU when we have positive output values.

Equation (\ref{eq:backprop}) are the basis for the backpropagating algorithm, described in Algorithm \ref{alg:backprop}.
\begin{algorithm}[htbp]\caption{The backpropagation algorithm.}\label{alg:backprop}
	\SetAlgoLined
	\BlankLine
	\BlankLine
	Compute $\{ \boldsymbol{a}^l\}_{l=1}^L$ with feed forward\;
	Compute $\delta^L$\;
	Set $\frac{\partial C}{\partial \boldsymbol{b}^L} = \delta^L$\;
	Compute $\frac{\partial C}{\partial \boldsymbol{w}^L} = \delta^L(\boldsymbol{a}^{L-1})^T$\;
	\ForEach{l=L-1:2}{
		Compute $\delta^l$\;
		Set $\frac{\partial C}{\partial \boldsymbol{b}^l} = \delta^l$\;
		Compute $\frac{\partial C}{\partial \boldsymbol{w}^l} = \delta^l(\boldsymbol{a}^{l-1})^T$\;
	}
	\BlankLine
	\BlankLine
	\end{algorithm}


\subsection{Data sets}
In this report, the default of credit card clients data set \citep{yeh2009UCI} was used to study the performance and compare the logistic regression method and neural networks. A description of the attributes of the data set can be found in \cite{yeh2009UCI}. Upon inspection, it is notable that the data set contains a considerable amount of values different from their valid values as described by \citeauthor{yeh2009UCI}. Mostly, this apply for the categorical variables, where the given value does not correspond to any category. By removing data points with invalid values, as well as entries where the client does not have any history of past payments or bill statements, the data set is reduced from 30000 data points to 3792 points. As this is a considerable reduction of data points, a second reduced data set was constructed, where (OGSÅ HVA VI GJORDE <---- HER!)

In addition the default of credit card clients data set, data produced with Franke's function was applied to neural networks. For details on Franke's function, see \cite{prosjekt1}.

\subsubsection*{Reduction of Dimention}
(HER!)

\subsection{Quality of Measurements}
(HER!)
