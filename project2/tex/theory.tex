\section{Theory and methods}
\label{sec:theory}

\subsection{Data sets}
\label{sec:datasets}
In this report, the default of credit card clients data set \citep{yeh2009UCI} was used to study the performance and compare the logistic regression method and neural networks for classification. A description of the attributes of the data set can be found in \cite{yeh2009UCI}. Upon inspection, it is notable that the data set contains a considerable amount of values different from their valid values as described by \citeauthor{yeh2009UCI}. Mostly, this apply for the categorical variables, where the given value does not correspond to any category. By removing data points with invalid values, as well as entries where the client does not have any history of past payments or bill statements, the data set is reduced from 30000 data points to 3792 points. As this is a considerable reduction of data points, two additional data sets were constructed. In one, all the invalid PAY values were kept. These invalid values were some $-2$s, but mostly $0$s. In the second additional set, all $-2$s, $-1$s and $0$s were set to $0$, since these were assumed to all represent duly payments. Both sets contained 26908 data points, with 78\% of the labels equal to zero, i.e. not default payment next month. The last set gave best results on initial runs, and was used for the rest of the project. For a more in-depth inspection of the data set, see the Jupyter notebook \texttt{inspect\_data.ipynb} in the \href{https://github.com/janadr/FYS-STK4155/tree/master/project2}{GitHub repository} of janadr.

In addition to the default of credit card clients data set, data produced with Franke's function was applied to neural networks. For details on Franke's function, see \cite{prosjekt1}.

\subsection{Logistic Regression}
Classification problems aim to predict the behaviour of a given object, and look for patterns based on discrete variables (i.e categories). Logistic Regression can be used to solve such problems, commonly by the use of variables with binary outcomes such as true/false, positive/negative, success/failure etc., or in the specific credit card case: \textit{risky/non-risky} \cite{LogRegLectures}

As opposed to Linear Regression, the equation one gets as a result of minimization of of the cost function by $\hat{\beta}$ using Logistic Regression, is non-linear, and is solved using minimization algorithms called \emph{gradient descent methods}. \\

When predicting the the output classes in which an object belongs, the prediction is based on the design matrix $\mathbf{\hat{X}} \in \mathbb{R}^{n\cross p}$ that contain $n$ samples that each carry $p$ features.

A distinction is made between \textit{hard classification} - deterministically determine the variable to a category, and \textit{soft classification} - determines the probability that a given variable belongs in a certain category. The latter is favourable in many cases, and logistic regression is the most used example og this type of classifier.

When using Logistic Regression, the probability that a given data point $x_i$ belongs in a category $y_i$ is given by the Sigmoid-function (or logistic function):
\begin{equation}
\begin{split}
    & p(t) = \frac{1}{1 + e^{-t}} = \frac{e^t}{1+e^t} \\
    & 1-p(t) = p(-t)
\end{split}
  \label{eq:sigmoid}
\end{equation}

Assuming a binary classification problem, i.e. $y_i$ can be either 0 or 1, and a set of predictors $\hat{\beta}$ the Sigmoid function \eqref{eq:sigmoid} gives the probabilities with relation:
\begin{equation*}
  p(y_i = 0|x_i,\hat{\beta})  = 1 - p(y_i = 1|x_i,\hat{\beta})
  \label{eq:prob_relation}
\end{equation*}

The total likelihood for all possible outcomes $\mathcal{D}=\{(y_i,x_i)\}$ is used in the Maximum Likelihood Estimation (MLE), aiming at maximizing the log/likelihood function \eqref{eq:log_p}. The likelihood function can be expressed with $\mathcal{D}$:
\begin{equation*}
\begin{split}
    &\qquad P(\mathcal{D}|\hat{\beta}) = \\
    &\prod_{i=1}^n \qty[p(y_i = 1|x_i,\hat{\beta}) ]^{y_i}  \qty[1 - p(y_i = 0|x_i,\hat{\beta})]^{1-y_i}
\end{split}
\end{equation*}

And the log/likelihood function is then:
\begin{equation}
\begin{split}
    &P_{\log}(\hat{\beta}) = \\
    &\sum_{i=1}^n \Big(y_i \log \qty[p(y_i = 1|x_i,\hat{\beta})] \\
    &\quad+ (1-y_i) \log [1 - p(y_i = 0|x_i,\hat{\beta})]\Big)
\end{split}
\label{eq:log_p}
\end{equation}

The cost/error-function $\mathcal{C}$ (also called cross-entropy in statistics) is the negative of the log/likelihood. Maximizing $P_{\log}$ is thus the same as minimizing the cost function. The cost function is:
\begin{equation}
  \begin{split}
    &\mathcal{C}(\hat{\beta}) = - P_{\log}(\hat{\beta}) =  \\
    -&\sum_{i=1}^n \Big(y_i \log \qty[p(y_i = 1|x_i,\hat{\beta})] \\
    &\quad+ (1-y_i) \log [1 - p(y_i = 0|x_i,\hat{\beta})]\Big)
  \end{split}
  \label{eq:cost_function}
\end{equation}

Finding the parameters $\hat{\beta}$ that minimize the cost function is then done through differentiation.
Defining the vector $\hat{y}$ containing $n$ elements $y_i$, the $n \cross p$ matrix $\hat{X}$ containing the $x_i$ elements, and the vector $\hat{p}$ that is the fitted probabilities $p(y_i|x_i,\hat{\beta})$, the first derivative of $\mathcal{C}$ is
\begin{equation}
  \nabla_\beta \mathcal{C} = \pdv{\mathcal{C}(\hat{\beta})}{\hat{\beta}} = - \hat{X}^T (\hat{y}-\hat{p})
  \label{eq:cost_d}
\end{equation}
This gives rise to set of linear equations, where the aim is to solve the system for $\hat{\beta}$.

With $\hat{x} = [1, x_1,x_2,...,x_p]$ and $p$ predictors $\hat{\beta} = [\beta_0,\beta_1,\beta_2,...,\beta_p]$ the ration between likelihoods of outcome is:
\begin{equation}
  \log \frac{p(\hat{\beta}\hat{x})}{1-p(\hat{\beta}\hat{x})} = \beta_0 + \beta_1x_1 + ... + \beta_px_p
  \label{eq:prob_ratio}
\end{equation}

\noindent and $p(\hat{\beta}\hat{x})$ defined by:
\begin{equation}
  p(\hat{\beta}\hat{x}) = \frac{e^{\beta_0 + \beta_1x_1 + ... + \beta_px_p}}{1+e^{\beta_0 + \beta_1x_1 + ... + \beta_px_p}}
  \label{eq:pBx}
\end{equation}

\subsection{Gradient Descent Methods}
\subsubsection*{The General Idea}
With the gradient of $\mathcal{C}$ defined as in \eqref{eq:cost_d}, we use this to find the minimum of the cost function. The basic idea is that by moving in the direction of the negative gradient of a function, we can move towards the value (in this case the $\beta$) that minimizes the function (in this case $\mathcal{C}(\beta)$). \cite{GDLectures}

This is done by repeating the algorithm
\begin{equation}
    \beta_{j+1} = \beta_j - \gamma \nabla_\beta \mathcal{C}(\beta) \quad j = 0,1,2,...
    \label{eq:gradient_decent}
\end{equation}
When a minimum is approached, $\nabla_\beta \mathcal{C}(\beta) \rightarrow$ 0, and thus we can set a limit when $\beta_{k+1} \approx \beta_k$ given a certain tolerance, and the $\beta$ which minimizes the cost function is found. $\gamma$ is in this case called the \textit{learning rate}, and is a parameter that must be tuned to each specific case in order to optimize the regression.

\subsubsection*{Stochastic Gradient Descent}
In this project we use a stochastic version of gradient descent, which is an improvement upon the regular gradient descent. This is done by expression the cost function (and thus also its gradient) as a sum
\begin{equation}
    \nabla_\beta \mathcal{C}(\beta) = \sum_i^n     \nabla_\beta c_i(\boldsymbol{x_i},\beta) ,
    \label{eq:gradient_sum}
\end{equation}
and by only taking calculating the gradient of a subset of the data at the time. These subsets, called \textit{minibatches} are of size $\boldsymbol{M}$ , and the total amount is $\frac{n}{\boldsymbol{M}}$ where $n$ is the amount of data points. The minibatches are denoted $\boldsymbol{B}_k$, with k = 1,2,...,$\frac{n}{\boldsymbol{M}}$.

Instead of a sum over all the the data points $i \in [1,n]$ we now in each step, sum over all the data points in the given minibatch $i \in \boldsymbol{B}_k$ where $k$ is picked randomly with uniform probability from $[1, \frac{n}{\boldsymbol{M}}]$.

The stochastic and final version of \eqref{eq:gradient_decent} is therefore given by the algorithm

\begin{equation}
    \beta_{j+1} = \beta_j - \gamma_j \sum_{i \in \boldsymbol{B}_k}\nabla_\beta c_i(\boldsymbol{x_i},\beta)
    \label{eq:stochstic_gradient_descent}
\end{equation}
An iteration over the total number of minibatches is commonly referred to as an \textit{epoch.}\\

By using the stochastic gradient descent method \eqref{eq:stochstic_gradient_descent} to minimize the cost function \eqref{eq:cost_function} we can this find the $\beta$ values that give the most accurate classification, by doing \textit{logistic regression}.

\subsubsection*{Dynamic learning rate}
Convergence rate might be slow, and a possible solution to this is changing learning rate as
the gradient descent gets closer to a local minima. Instead of a static learning rate, we have a learning schedule. As we expect convergence as iterations increase, it is natural that the learning schedule
be a function of epochs and minibatches. Specifically, the learning rate should decrease as it approaches a minima.
A possible function for the learning rate is then
  \begin{equation}
    \label{eq:learning_schedule}
    \Gamma_{i} = \frac{\Gamma_0}{nN+i},
  \end{equation}
where $\Gamma_{0}$ is the initial learning rate, n the current epoch, i the current minibatch,
and N the minibatch size.

\subsection{Neural Networks}
In this section, the equations used are based off the book by \cite{Nielsen}, unless otherwise specified.
\subsubsection*{The structure of a network}
Neural Networks, as the name suggests, are inspired by our understanding of how networks of neurons function in the brain. As can be seen in the example network in Figure \ref{fig:NNstructure}, neurons are structured in layers. We always have a input and an output layer, in addition to a varying number of hidden layers. The input layer has as many neurons as there are input variables, while the output layer has one neuron for each output. How many neurons you have in the output layer depends on the specific problem. The number of neurons in each hidden layer, on the other hand, is not directly related to inputs or outputs, and must be decided in some other way.

As the diagram in Figure \ref{fig:NNstructure} suggests, the neurons in each layer are not connected with each other, but takes in inputs from the previous layer and passes on an output to the neurons in the next layer, as illustrated with arrows. This way, the inputs are fed through the network and processed, resulting in an output.
\begin{figure}[htbp]
	\centering
	\begin{tikzpicture}[
	plain/.style={
		draw=none,
		fill=none,
	},
	net/.style={
		matrix of nodes,
		nodes={
			draw,
			circle,
			inner sep=8pt
		},
		nodes in empty cells,
		column sep=0cm,
		row sep=-9pt
	},
	>=latex
	]
	\matrix[net] (mat)
	{
		|[plain]| \parbox{0.8cm}{\centering Input\\layer} & |[plain]| \parbox{0.8cm}{\centering Hidden\\layer} & |[plain]| \parbox{0.8cm}{\centering Output\\layer} \\
		& |[plain]| \\
		|[plain]| & \\
		& |[plain]| \\
		|[plain]| & |[plain]| \\
		& & \\
		|[plain]| & |[plain]| \\
		& |[plain]| \\
		|[plain]| & \\
		& |[plain]| \\    };
	\foreach \ai [count=\mi ]in {2,4,...,10}
	\draw[<-] (mat-\ai-1) -- node[above] {Input \mi} +(-1.9cm,0);
	\foreach \ai in {2,4,...,10}
	{\foreach \aii in {3,6,9}
		\draw[->] (mat-\ai-1) -- (mat-\aii-2);
	}
	\foreach \ai in {3,6,9}
	\draw[->] (mat-\ai-2) -- (mat-6-3);
	\draw[->] (mat-6-3) -- node[above] {Output} +(1.9cm,0);
	\end{tikzpicture}
	\caption{Schematic diagram of a neural network with five input neurons in the input layer, one hidden layer with tree neurons and a single output neuron in the output layer.}
	\label{fig:NNstructure}
\end{figure}

\subsubsection*{Forward feeding}
Each neuron has one or multiple inputs, as illustrated with arrows in Figure \ref{fig:NNstructure}. Each of these inputs has a weight associated with it. To clarify the notation used, let's take a look at the $j$th neuron in the $l$th layer. The weight associated with the input coming from the $k$th neuron in the previous layer is denoted as $w^l_{jk}$. In addition, each neuron has a bias associated with it, for the neuron in question denoted as $b^l_j$. Summing the weighted inputs and the bias, and feeding this to a function $f$, gives the activation $a^l_j$:
\begin{equation*}
	a^l_j = f\left(\left(\sum_{k}w^l_{jk}a^{l-1}_k\right) + b^l_j\right)
\end{equation*}
This activation is then fed forward as input to all the neuron in the next layer.

In matrix notation, the activation for the whole layer $l$ can be written as
\begin{equation}\label{eq:forward}
	\boldsymbol{a}^l = f\left(\boldsymbol{w}^l\boldsymbol{a}^{l-1}+\boldsymbol{b}^l\right)
\end{equation}
Here, $\boldsymbol{a}^l$ and $\boldsymbol{b}^l$ are vertical vectors containing the activations and biases of the $l$th layer, while $\boldsymbol{w}^l$ is a matrix with elements $w^l_{jk}$, i.e. the $j$th column contains the weights of the inputs reaching the $j$th neuron.

Let's look at the activation function in Eq. (\ref{eq:forward}) denoted with a $f$. In the case of classification, the sigmoid function stated in Eq. (\ref{eq:sigmoid}) is often used in introductory texts. As we will see in the backpropagation algorithm, the sigmoid is a good choice for activation function, since a small change in the output can be propagated backwards, resulting in small changes in the weights and biases through the network.

Another activation function is the so-called rectified linear unit (ReLU) function, given as $f(z)=\max{\left(0,z\right)}$ This gives an activation function which only fires when $z$ is positive. In the case of regression, a variant of ReLU, called leaky ReLU, was used in this project. This function is given as
\begin{equation}\label{leakyReLU}
  f(z) = \begin{cases}
            z &z\geq 0 \\
            0.01z &z<0 \\
         \end{cases}
\end{equation}
This form of ReLU has a small positive gradient for negative values, which allows for gradient based learning even when the $z$-value is negative \citep{wang2018classification}.

With a basis in Eq. (\ref{eq:forward}), the algorithm for forward feeding is given in Algorithm \ref{alg:forward}. Here $L$ is the total number of layers.
\begin{algorithm}[htbp]\caption{The forward feeding algorithm.}\label{alg:forward}
	\SetAlgoLined
	\BlankLine
	\BlankLine
	Set $\boldsymbol{a}^1$ = input\;
	\ForEach{l=2:L}{
    Compute $\boldsymbol{z}=\boldsymbol{w}^l\boldsymbol{a}^{l-1}+\boldsymbol{b}^l$\;
		Compute  $\boldsymbol{a}^l=f(\boldsymbol{z})$\;}
	Set output to $\boldsymbol{a}^L$\;
	\BlankLine
	\BlankLine
\end{algorithm}

Note that the output will have values between 0 and 1, when the sigmoid function is used to compute the activations of all the layers. In a classification problem, this corresponds to the likelihood of an outcome. For example, in a classification problem with five classes, the network would have five output neurons, each representing a class. The final classification of an input would then be the class with the highest probability.
In the regression case, we see that when using the leaky ReLU activation function from Eq. (\ref{leakyReLU}), the output can be any positive value (strictly also negative values, due to the leakage, but this would call for large negative $z$-values).

\subsubsection*{Backpropagation}
When training the network, the goal is to find the weights and biases that minimize the cost function $C$. In this project, the cross-entropy cost function was used for classification. This is given as
\begin{equation}\label{eq:cross-entropy}
	C = -\sum_{i=1}^n\left[y_i\ln a^L_i + (1-y_i)\ln(1-a^L_i)\right]
\end{equation}
Here, we are summing over $n$ points of training data, where $a_i^L$ is the output with the corresponding correct value $y_i$.

In the regression case, the quadratic cost function, given as
\begin{equation}\label{quadratic cost}
  C = \frac{1}{2}\sum_{i=1}^n||y_i-a_i^L||^2
\end{equation}
is used.

To find the weights and biases that minimize Eq.(\ref{eq:cross-entropy}) and (\ref{quadratic cost}), one can use Stochastic Gradient Decent, as described previously. But in order to use SGD, the derivatives of $C$ with respect to all the weights and biases must be computed, and it is here that backpropagation comes in. It can be shown, by repeating the chain rule, that the derivatives are given as in Eq. (\ref{eq:backprop}).
\begin{equation}\label{eq:backprop}
\begin{aligned}
	\delta^L &= \nabla_aC\odot f'(z^L)\\
	\delta^l &= ((\boldsymbol{w}^{l+1})^T\delta^{l+1})\odot f'(\boldsymbol{z}^l) \\
	\frac{\partial C}{\partial b^l_j} &= \delta_j^l \\
	\frac{\partial C}{\partial w_{jk}^l} &= a_k^{l-1}\delta^l_j
\end{aligned}
\end{equation}

Taking a look at the expression for $\delta^L$, one can show that this reduces to $\delta^L = \boldsymbol{a}-\boldsymbol{y}$, when the cross-entropy cost function is combined with the sigmoid activation function, and similarly for the quadratic cost and ReLU when we have positive output values.

Equation (\ref{eq:backprop}) are the basis for the backpropagating algorithm, described in Algorithm \ref{alg:backprop}.
\begin{algorithm}[htbp]\caption{The backpropagation algorithm.}\label{alg:backprop}
	\SetAlgoLined
	\BlankLine
	\BlankLine
	Compute $\{ \boldsymbol{a}^l\}_{l=1}^L$ with feed forward\;
	Compute $\delta^L$\;
	Set $\frac{\partial C}{\partial \boldsymbol{b}^L} = \delta^L$\;
	Compute $\frac{\partial C}{\partial \boldsymbol{w}^L} = \delta^L(\boldsymbol{a}^{L-1})^T$\;
	\ForEach{l=L-1:2}{
		Compute $\delta^l$\;
		Set $\frac{\partial C}{\partial \boldsymbol{b}^l} = \delta^l$\;
		Compute $\frac{\partial C}{\partial \boldsymbol{w}^l} = \delta^l(\boldsymbol{a}^{l-1})^T$\;
	}
	\BlankLine
	\BlankLine
	\end{algorithm}

\subsubsection*{Overfitting and Regularization}
A problem that can arise when fitting a regression model, as discussed in \cite{prosjekt1}, is overfitting to data. This is a major problem also in neural networks, where the network can fit too heavily to outliers in the training set, especially when you have a large number of neurons. Apart from increasing the training data set and decreasing the number of neurons, a third approach is to use a regularization technique. Here, we will use the weight decay or L2 regularization technique.
The idea is to add a term to the cost function. For both cross-entropy and quadratic cost, the new cost function can be written as
\begin{equation*}
  C = C_0 + \frac{\lambda}{2n}\sum_w w^2
\end{equation*}
where $C_0$ is the original cost function and $\lambda>0$ is the regularization parameter.
The algorithm for updating the weights, when using stochastic gradient descent, then becomes
\begin{equation}\label{eq:w_update}
  w \rightarrow \left(1-\frac{\gamma\lambda}{n}\right)w -  \frac{\gamma}{m}\sum_{i \in \boldsymbol{B}_k}\frac{\partial C_i}{\partial w}
\end{equation}
Here, $m$ is the mini batch size. The algorithm for updating the biases are unchanged:
\begin{equation}\label{eq:b_update}
  b \rightarrow b -  \frac{\gamma}{m}\sum_{i \in \boldsymbol{B}_k}\frac{\partial C_i}{\partial b}
\end{equation}

Note the factor $m^{-1}$ in Eq. (\ref{eq:w_update}) and (\ref{eq:b_update}) when comparing to Eq. (\ref{eq:stochstic_gradient_descent}). This factor can be understood as taking the mean of all the gradients, instead of the sum. In this project, the sum was used for logistic regression, while the mean was used in the neural network. In practise, this corresponds to a scaling of the learning rate.

\subsection{Quality of Measurements}
To measure how good the different methods were at classifying the credit card data, the data was split into a training set and a test set. After training the model on the test set, it was applied on the training set, and a performance score was calculated. Both accuracy, given in Eq. (\ref{eq:acc}) and the area under the receiver operating characteristic curve, denoted AUC, was applied.
\begin{equation}\label{eq:acc}
  \text{Accuracy} = \frac{\text{Correct classifications}}{\text{Total \# of classifications}}
\end{equation}
For calculations of AUC, \texttt{sklearn.metrics.roc\_auc\_score} was used. AUC can be interpreted as the probability that a random true positive sample is ranked higher than a random true negative. This score is not dependent on the chosen classification threshold, i.e. the value that must be exceed to classify an input as one instead of zero.

For data generated with Franke's function, the same test-train-split was made, and the mean squared error was calculated, as in \cite{prosjekt1}.

\subsection{Implementation}
All our algorithms were implemented in python3.6 using the numpy, matplotlib, pandas, seaborn,
and scikit-learn packages. We wrote our own class "Regression" for linear and logistic
regression, as well as class "Network" for neural network. In the way our neural network was implemented, we could use the same code for classification and regression by simply replacing activation functions, while at the same time ensuring that the appropriate cost function was used.
Scikit-learn was used to verify our implementation by comparing accuracy and AUC scores.
Code along with data and figures can be found on our github repository under "Project 2".\footnote{https://github.com/janadr/FYS-STK4155.git}
