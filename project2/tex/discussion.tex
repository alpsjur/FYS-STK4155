\section{Discussion}
\label{sec:discussion}

\subsection{Tuning to optimal parameters}
As previously stated, two measurements were used to detemine the quality of the different methods for classification. Accuracy as described in \eqref{eq:acc}, and the AUC ROC score. The AUC ROC is a good measurement, not only because it is independant of the classifier threshold, but also, as we can see from figure \ref{fig:TuneLogReg_auc} and \ref{fig:TuneNN_auc}, it gives a more nuanced view of the belts in which the optimal regularisation parameters are located. For both Logistic Regression and Neural Network, the areas of the heatmaps that give the best accuracy are more or less the same that give the best AUC ROC score, however for the two classifiers the behaviour is rather different.

For the Logistic Regression, we see an increase in the quality of the classifier with increased initial learning rate $\gamma$ and increased batch size. If the learning rate becoms too large however, overflow is encountered, and we get invalid results. The fluctuations of the accuracy and AUC ROC within the yellow belts in figure \ref{fig:TuneLogReg_accuracy} and \ref{fig:TuneLogReg_auc} are relatively small, and as long as the parameters are chosen from this area, we can expect results close the the optimal. The true optimal parameters for accuracy and ROC AUC score are presented in table \ref{tab:acc} and \ref{tab:AUC}, and show that the best results we could obtain with the Logistic Regression classifier, was an accuracy of 82.0 \% and an AUC score of 0.765. This accuracy corresponds nicely to the accuracy calculated with the logistic regression provided by scikit-learn.

For the Neural Network classifier, we see whay appears to be an opposite behaviour of the Logistic Regression. The classifier produces the best results for smaller batch sizes, and a smaller learning rate. Also in this case, the impression is that, as long as we choose parameters from within the yellow belt in figure \ref{fig:TuneNN_accuracy} and \ref{fig:TuneNN_auc}, the difference in the quality is relatively small, and we are close to the optimal behaviour of the classifier. For the neural network, with the parameters as described, again in table \ref{tab:AUC} and \ref{tab:acc} we are able to get an accuracy of 82.3 \% and an AUC ROC score of 0.782, which is an accuracy that just like for Logistic Regression, corresponds well with the results produced by scikit-learns neural network, and thus strengthens our results.

\subsubsection{Comparing the classifiers}
For the accuracy the two classifiers obtain very similar results, with Neural Network only slightly better than Logisic Regression. When comparing the AUC ROC score, however, the Neural Network classifier severely outperforms the Logistic Regression one. This is another reason why the score is an important measurement to include. Taking this in to consideration it becomes clear that Neural Network is the better option for this case. When looking at the confusion matrix in table \ref{tab:confusion}, we also see that the network is better at identifying the negative results than the Logistic Regression, and in this case, where the negative results is a minority, that becomes important. That Neural Network is the better option, is also the conclusion that is reached in \cite{yeh2009UCI}. Here their error rate of (HER!) also corresponds well with our calculated accuracy of 82 \%. Regarding the AUC ROC score, this cannot be directly compared as they have used Area Ratio rather than ROC AUC. What is clear however, is that it is when looking at the optimal curve and area scores that one truly sees that Neural Networks are better, as this was the case both for us, and \cite{yeh2009UCI}.

\subsection{Franke?}
\begin{table}[htbp]
	\renewcommand{\arraystretch}{1.2}
	\centering
	\caption{The minimum MSE achieved when applying different regression models and Neural Network on Franke's function.}
	\begin{tabular}{l l}
		\toprule
		Method & MSE \\
		\midrule
		Ordinary Least Square & 0.017 \\
		Ridge regression & 0.015\\
		Lasso regression & 0.021\\
		Neural Network & 0.011\\
		\bottomrule
	\end{tabular}
	\label{tab:mse}
\end{table}
