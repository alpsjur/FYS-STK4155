\section{Discussion}
\label{sec:discussion}

\subsection{Tuning to optimal parameters}
As previously stated, two measurements were used to detemine the quality of the different methods for classification. Accuracy as described in \eqref{eq:acc}, and the AUC-score. The AUC-score is a good measurement, not only because it is independant of the classifier threshold, but also, as we can see from figure \ref{fig:TuneLogReg_auc} and \ref{fig:TuneNN_auc}, it gives a more nuanced view of the belts in which the optimal regularisation parameters are located. For both Logistic Regression and Neural Network, the areas of the heatmaps that give the best accuracy are more or less the same that give the best AUC-score, however for the two classifiers the behaviour is rather different.

For the Logistic Regression, we see an increase in the quality of the classifier with increased initial learning rate $\gamma$ and increased batch size. If the learning rate becomes too large however, overflow is encountered, and we get invalid results. The fluctuations of the accuracy and AUC-score within the yellow belts in figure \ref{fig:TuneLogReg_accuracy} and \ref{fig:TuneLogReg_auc} are relatively small, and as long as the parameters are chosen from this area, we can expect results close the the optimal. The true optimal parameters for accuracy and AUC-score are presented in table \ref{tab:acc} and \ref{tab:AUC}, and show that the best results we could obtain with the Logistic Regression classifier, was an accuracy of 0.820 and an AUC score of 0.765. This accuracy corresponds nicely to the accuracy calculated with the logistic regression provided by scikit-learn.

For the Neural Network classifier, we see whay appears to be an opposite behaviour of the Logistic Regression. The classifier produces the best results for smaller batch sizes, and a smaller learning rate. Also in this case, the impression is that, as long as we choose parameters from within the yellow belt in figure \ref{fig:TuneNN_accuracy} and \ref{fig:TuneNN_auc}, the difference in the quality is relatively small, and we are close to the optimal behaviour of the classifier. For the neural network, with the parameters as described, again in table \ref{tab:AUC} and \ref{tab:acc} we are able to get an accuracy of 0.823 and an AUC-score of 0.782, which is an accuracy that just like for Logistic Regression, corresponds well with the results produced by scikit-learns neural network, and thus strengthens our results.

One explanation of the different behaviour of the two classifiers are the difference in how the stochastic gradient decent is implemented, as described in the Theory section. This result in different scaling of the learning rate, as the minibatch size changes. 

\subsubsection{Comparing the classifiers}
For the accuracy the two classifiers obtain very similar results, with Neural Network only slightly better than Logisic Regression. When comparing the AUC score, however, the Neural Network classifier outperforms the Logistic Regression by 0.017 points. This is another reason why the score is an important measurement to include. Taking this in to consideration it becomes clear that Neural Network is the better option for this case. When looking at the confusion matrix in table \ref{tab:confusion}, we also see that the network is better than the Logistic Regression at identifying both the default and non-default payments, and in this case, where the negative results is a minority, that becomes important. That Neural Network is the better option, is also the conclusion that is reached in \cite{yeh2009UCI}. Here their error rate of 0.17 also corresponds well with our calculated accuracy of 0.82. Regarding the AUC-score, this cannot be directly compared as they have used Area Ratio rather than the AUC-score. What is clear however, is that it is when looking at the optimal curve and area scores that one truly sees that Neural Networks are better, as this was the case both for us, and \cite{yeh2009UCI}.

However, looking at the true positives and false negatives in \ref{tab:confusion}, our Neural Network is quite bad at identifying positive instances. As we see that there are more false negatives than true positives, most of the positives, i.e. default payments, are not correctly classified. This could be due to the small fraction of default payments in the data set, which means that the data set itself is biased. One possible way of dealing with this could be to resample the data set, so that the network would train on an equal amount of positive and negative instances. Another possibility, which was not explored in this project, would be to tune the threshold, so that more outputs would be classified as positive. 

\subsection{Comparing methods for regression}
Shown in Table \ref{tab:mse} is a comparison between the MSE obtained from our neural network and other values obtained from linear regression algorithms. We see that the neural network performs better compared with any other method. A suggestion for why this is the case is that the Franke function is not a polynomial. The neural network, however, constructs a general function, as discussed by \citet{Nielsen}, and so is not bound by assumption of the design matrix on our part. With that said, the choice of activation function can have a large impact, and we found that the leaky ReLU was the only one that converged with reasonable tuning.
\begin{table}[htbp]
	\renewcommand{\arraystretch}{1.2}
	\centering
	\caption{From descending order, the first three values were collected from \citet{prosjekt1} and represent the lowest MSE achieved from the linear regression algorithms with complexity 6, 9, and 11, respectively. The last value is the MSE obtained from our neural network.}
	\begin{tabular}{l l}
		\toprule
		Method & MSE \\
		\midrule
		Ordinary Least Square & 0.017 \\
		Ridge regression & 0.015\\
		Lasso regression & 0.021\\
		Neural Network & 0.011\\
		\bottomrule
	\end{tabular}
	\label{tab:mse}
\end{table}
