\section{Discussion}
\label{sec:discussion}

Our work shows that pde's can be solved using neural networks (Figure \ref{fig:NNcompare}), yielding similar results as a finite difference approach, i.e. Figure \ref{fig:FDcompare}. Looking back at Figure \ref{fig:MSEbench} we see that the two methods yields approximately the same MSE after 100000 time steps and 10000 iterations respectively. However, Figure \ref{fig:MSEbench}a. and \ref{fig:MSEbench}d. shows a more consistent decrease compared with Figure \ref{fig:MSEbench}b. and \ref{fig:MSEbench}d. Furthermore, the shape of the curve in a. and c. looks similar, while there is a difference between b. and d. Figure \ref{fig:MSE_T2E-02_varying_gamma} and \ref{fig:MSE_T3E-01_varying_gamma} also indicates that this difference is dependent on the chosen learning rate. This might be expected due to the cost function of the neural network being dependent on the shape of the curve, which is different in time.

We also studied the effect of varying time interval for the neural network and found that the overall trend was dominated by the number of iterations (see Figure \ref{fig:MSE_varying_dt}). One interesting point, however, is that $N_t=50$ performed better than $N_t=100$, and so there isn't a linear relationship between increasing $N_t$ and decreasing MSE. In the FE/CD scheme there is a stability criterion between $\Delta t$ and $\Delta x$ which is taken into account. There is no such criterion for the neural network, but our results in Figure \ref{fig:MSE_T2E-02_varying_gamma} might indicate that the neural network considers this relationship.

Comparing the two methods in CPU time, we see that the neural network starts out at being around 3 orders of magnitude, if not more, slower than the finite difference methods. After 100000 time points and 10000 iterations, however, they are around the same. This was also intentional as we wanted to present a fair comparison in MSE as a function of CPU time. Looking at the inclines, it looks as if the increase in CPU time for the neural network is decreasing, while for the finite difference method approach, it is increasing at a constant rate. With more computing power, the neural network might win out in an MSE per CPU time perspective.

While we have shown that neural networks can solve pde's at a level of accuracy close to a finite difference method, the results in this study are not enough to conclude which approach yields the best results. Future studies can choose more sophisticated finite difference methods such as higher order Runge-Kutta, which could yield a smaller MSE. Furthermore, varying the size of the time step could speed up the algorithm where possible and slowing it down where needed. Likewise, for the neural network, an adaptive learning rate can be implemented, which might speed up convergence and mitigate difference in MSE between target times. More of the hyperparameters can also be studied, such as the amount of hidden layers and neurons.

When it comes to calculating eigenpairs with the neural network, it seems to yield good results that agrees with the \texttt{numpy.linalg} method, when it comes to precision. In this report, the network was set to run until the cost function went below a threshold of $10^{-4}$. When experimenting with the network, it was clear that there was a direct link between the threshold of the cost function, and the level of agreement between the network results and the numpy results. In other words, it seems to be possible to tune the network corresponding to the level of precision needed. However, the higher the precision, the more iterations is needed.

The maybe biggest drawback of the neural network is it's time use, with a CPU time about 1000 times that of \texttt{numpy.linalg} for a precision of $10^{-4}$ for the largest eigenvalue. The smallest eigenvalue demanded even more iterations to reach that precision. This leads us to another feature of the neural network. As can be seen in Figure \ref{fig:eigenvector_max} and \ref{fig:eigenvector_min}, the time $t$ at which the estimated eigenvectors stabilises varies from matrix to matrix, and the number of iterations needed varies as well. So although it is possible to compute eigenpairs with a high level of precision, the computation can possibly become very time consuming.

Lastly, this methods only gives us the largest and the smallest eigenvalues, together with the corresponding eigenvectors. In other words, this method can not be used to find all the eigenpairs of a real symmetric matrix.
