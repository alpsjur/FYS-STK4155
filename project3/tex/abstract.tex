\begin{abstract}

In this paper we use a neural network approach for solving the diffusion equation and compare this with a traditional finite difference approach using Forward Euler (FE) and Centered Difference (CD) methods. We also find eigenpairs of a real symmetric matrix using a similar neural network approach. Our findings show that neural networks can be used to solve partial differential equations. We find that the neural network compares to the finite difference approach, but starts out 3 orders of magnitude slower, reaching the same run time for the same MSE after $10^5$ time steps and $10^4$ iterations, respectively. We also looked at the effect of learning rate on MSE, and detected a possible minimum at $0.006$. While temporal resolution had an effect on the neural network, we found that the overall trend was dominated by increasing iterations. When computing eigenvalues, we found that the neural network gave results that differed from \texttt{numpy.linalg} first at the 5th decimal place, and the eigenvectors differed first at the 3th decimal place, when the threshold of the cost function was set to $10^{-4}$. We also find that the network was considerably slower than the numpy approach, with the convergence time and needed iterations seemingly dependent on the matrix. 
\end{abstract}
